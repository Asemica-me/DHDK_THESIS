
@misc{noauthor_open_2025,
	title = {Open {Source} vs. {Proprietary} {LLMs}: {A} {Comprehensive} {Comparison}},
	url = {https://www.hostcomm.co.uk/blogs/open-source-vs-proprietary-llms-a-comprehensive-comparison},
	urldate = {2025-06-19},
	journal = {Hostcomm Ltd.},
	month = jan,
	year = {2025},
}

@misc{noauthor_ibm_2025,
	title = {{IBM} {Watson}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=IBM_Watson&oldid=1301611671},
	abstract = {IBM Watson is a computer system capable of answering questions posed in natural language. It was developed as a part of IBM's DeepQA project by a research team, led by principal investigator David Ferrucci. Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.
The computer system was initially developed to answer questions on the popular quiz show Jeopardy! and in 2011, the Watson computer system competed on Jeopardy! against champions Brad Rutter and Ken Jennings, winning the first-place prize of US\$1 million.
In February 2013, IBM announced that Watson's first commercial application would be for utilization management decisions in lung cancer treatment, at Memorial Sloan Kettering Cancer Center, New York City, in conjunction with WellPoint (now Elevance Health).},
	language = {en},
	urldate = {2025-07-26},
	journal = {Wikipedia},
	month = jul,
	year = {2025},
	note = {Page Version ID: 1301611671},
}

@misc{noauthor_langchain_2024,
	title = {{LangChain} {Documentation} v0.3},
	url = {https://python.langchain.com/docs/introduction/},
	abstract = {LangChain is a framework for developing applications powered by large language models (LLMs).},
	language = {en},
	urldate = {2025-06-19},
	journal = {LangChain},
	year = {2024},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@misc{qu_bert_2019,
	title = {{BERT} with {History} {Answer} {Embedding} for {Conversational} {Question} {Answering}},
	url = {http://arxiv.org/abs/1905.05412},
	doi = {10.48550/arXiv.1905.05412},
	abstract = {Conversational search is an emerging topic in the information retrieval community. One of the major challenges to multi-turn conversational search is to model the conversation history to answer the current question. Existing methods either prepend history turns to the current question or use complicated attention mechanisms to model the history. We propose a conceptually simple yet highly effective approach referred to as history answer embedding. It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on BERT (Bidirectional Encoder Representations from Transformers). We first explain our view that ConvQA is a simplified but concrete setting of conversational search, and then we provide a general framework to solve ConvQA. We further demonstrate the effectiveness of our approach under this framework. Finally, we analyze the impact of different numbers of history turns under different settings to provide new insights into conversation history modeling in ConvQA.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Qu, Chen and Yang, Liu and Qiu, Minghui and Croft, W. Bruce and Zhang, Yongfeng and Iyyer, Mohit},
	month = oct,
	year = {2019},
	note = {arXiv:1905.05412},
	keywords = {Computer Science - Information Retrieval},
}

@article{zaib_conversational_2022,
	title = {Conversational question answering: a survey},
	volume = {64},
	issn = {0219-1377, 0219-3116},
	shorttitle = {Conversational question answering},
	url = {https://link.springer.com/10.1007/s10115-022-01744-y},
	doi = {10.1007/s10115-022-01744-y},
	abstract = {Abstract 
             
              Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on 
              conversational question answering 
              (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy a user’s information needs. While the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers over the recent years. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA.},
	language = {en},
	number = {12},
	urldate = {2025-07-26},
	journal = {Knowledge and Information Systems},
	author = {Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Zhang, Yang},
	month = dec,
	year = {2022},
	pages = {3151--3195},
}

@misc{chan_modern_2019,
	title = {Modern {Question} {Answering} {Systems} {Explained} {\textbar} deepset {Blog}},
	url = {https://www.deepset.ai/blog/modern-question-answering-systems-explained},
	abstract = {Let’s walk through the main stages of a modern question answering system and demystify it a bit so that you can start to build your own one.},
	language = {en},
	urldate = {2025-07-26},
	journal = {deepset},
	author = {Chan, Branden},
	month = nov,
	year = {2019},
}

@misc{xiong_dynamic_2018,
	title = {Dynamic {Coattention} {Networks} {For} {Question} {Answering}},
	url = {http://arxiv.org/abs/1611.01604},
	doi = {10.48550/arXiv.1611.01604},
	abstract = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0\% F1 to 75.9\%, while a DCN ensemble obtains 80.4\% F1.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
	month = mar,
	year = {2018},
	note = {arXiv:1611.01604},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yue_survey_2025,
	title = {A {Survey} of {Large} {Language} {Model} {Agents} for {Question} {Answering}},
	url = {http://arxiv.org/abs/2503.19213},
	doi = {10.48550/arXiv.2503.19213},
	abstract = {This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Yue, Murong},
	month = mar,
	year = {2025},
	note = {arXiv:2503.19213},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@article{nassiri_transformer_2023,
	title = {Transformer models used for text-based question answering systems},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04052-8},
	doi = {10.1007/s10489-022-04052-8},
	language = {en},
	number = {9},
	urldate = {2025-07-26},
	journal = {Applied Intelligence},
	author = {Nassiri, Khalid and Akhloufi, Moulay},
	month = may,
	year = {2023},
	pages = {10602--10635},
}

@misc{alsubhi_pre-trained_2021,
	title = {Pre-trained {Transformer}-{Based} {Approach} for {Arabic} {Question} {Answering} : {A} {Comparative} {Study}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Pre-trained {Transformer}-{Based} {Approach} for {Arabic} {Question} {Answering}},
	url = {https://arxiv.org/abs/2111.05671},
	doi = {10.48550/ARXIV.2111.05671},
	abstract = {Question answering(QA) is one of the most challenging yet widely investigated problems in Natural Language Processing (NLP). Question-answering (QA) systems try to produce answers for given questions. These answers can be generated from unstructured or structured text. Hence, QA is considered an important research area that can be used in evaluating text understanding systems. A large volume of QA studies was devoted to the English language, investigating the most advanced techniques and achieving state-of-the-art results. However, research efforts in the Arabic question-answering progress at a considerably slower pace due to the scarcity of research efforts in Arabic QA and the lack of large benchmark datasets. Recently many pre-trained language models provided high performance in many Arabic NLP problems. In this work, we evaluate the state-of-the-art pre-trained transformers models for Arabic QA using four reading comprehension datasets which are Arabic-SQuAD, ARCD, AQAD, and TyDiQA-GoldP datasets. We fine-tuned and compared the performance of the AraBERTv2-base model, AraBERTv0.2-large model, and AraELECTRA model. In the last, we provide an analysis to understand and interpret the low-performance results obtained by some models.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Alsubhi, Kholoud and Jamal, Amani and Alhothali, Areej},
	year = {2021},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{yoon_pre-trained_2019,
	title = {Pre-trained {Language} {Model} for {Biomedical} {Question} {Answering}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1909.08229},
	doi = {10.48550/ARXIV.1909.08229},
	abstract = {The recent success of question answering systems is largely attributed to pre-trained language models. However, as language models are mostly pre-trained on general domain corpora such as Wikipedia, they often have difficulty in understanding biomedical questions. In this paper, we investigate the performance of BioBERT, a pre-trained biomedical language model, in answering biomedical questions including factoid, list, and yes/no type questions. BioBERT uses almost the same structure across various question types and achieved the best performance in the 7th BioASQ Challenge (Task 7b, Phase B). BioBERT pre-trained on SQuAD or SQuAD 2.0 easily outperformed previous state-of-the-art models. BioBERT obtains the best performance when it uses the appropriate pre-/post-processing strategies for questions, passages, and answers.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Yoon, Wonjin and Lee, Jinhyuk and Kim, Donghyeon and Jeong, Minbyul and Kang, Jaewoo},
	year = {2019},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@incollection{garg_evolution_2023,
	address = {Boca Raton},
	edition = {1},
	title = {Evolution of question-answering system from information retrieval},
	isbn = {9781003244332},
	url = {https://www.taylorfrancis.com/books/9781003244332/chapters/10.1201/9781003244332-5},
	language = {en},
	urldate = {2025-07-26},
	booktitle = {Natural {Language} {Processing} and {Information} {Retrieval}},
	publisher = {CRC Press},
	author = {Das, Arijit and Saha, Diganta},
	collaborator = {Garg, Muskan and Kumar, Sandeep and Khader Jilani Saudagar, Abdul},
	month = oct,
	year = {2023},
	doi = {10.1201/9781003244332-5},
	pages = {110--140},
}

@article{hirschman_natural_2001,
	title = {Natural language question answering: the view from here},
	volume = {7},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {1351-3249, 1469-8110},
	shorttitle = {Natural language question answering},
	url = {https://www.cambridge.org/core/product/identifier/S1351324901002807/type/journal_article},
	doi = {10.1017/S1351324901002807},
	abstract = {As users struggle to navigate the wealth of on-line information now available, the  
need for automated question answering systems becomes more urgent. We need  
systems that allow a user to ask a question in everyday language and receive an  
answer quickly and succinctly, with sufficient context to validate the answer. Current  
search engines can return ranked lists of documents, but they do not deliver 
              answers 
              to the user. 
             
            Question answering systems address this problem. Recent successes have been  
reported in a series of question-answering evaluations that started in 1999 as part  
of the Text Retrieval Conference (TREC). The best systems are now able to answer  
more than two thirds of factual questions in this evaluation.},
	language = {en},
	number = {4},
	urldate = {2025-07-26},
	journal = {Natural Language Engineering},
	author = {Hirschman, L. and Gaizauskas, R.},
	month = dec,
	year = {2001},
	pages = {275--300},
}

@article{farea_understanding_2025,
	title = {Understanding question-answering systems: {Evolution}, applications, trends, and challenges},
	volume = {156},
	issn = {09521976},
	shorttitle = {Understanding question-answering systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197625009972},
	doi = {10.1016/j.engappai.2025.110997},
	language = {en},
	urldate = {2025-07-26},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Farea, Amer and Emmert-Streib, Frank},
	month = sep,
	year = {2025},
	pages = {110997},
}

@inproceedings{king_saud_university_riyadh_saudi_arabia_question_2019,
	title = {Question {Answering} {Systems} {Approaches} and {Challenges}},
	url = {https://acl-bg.org/proceedings/2019/RANLPStud%202019/pdf/RANLPStud011.pdf},
	doi = {10.26615/issn.2603-2821.2019_011},
	urldate = {2025-07-26},
	booktitle = {Proceedings of the {Student} {Research} {Workshop} {Associated} with {RANLP} 2019},
	publisher = {Incoma Ltd.},
	author = {{King Saud University, Riyadh, Saudi Arabia} and Alqifari, Reem and {University of York / York, UK}},
	month = sep,
	year = {2019},
	pages = {69--75},
}

@misc{liu_challenges_2022,
	title = {Challenges in {Generalization} in {Open} {Domain} {Question} {Answering}},
	url = {http://arxiv.org/abs/2109.01156},
	doi = {10.48550/arXiv.2109.01156},
	abstract = {Recent work on Open Domain Question Answering has shown that there is a large discrepancy in model performance between novel test questions and those that largely overlap with training questions. However, it is unclear which aspects of novel questions make them challenging. Drawing upon studies on systematic generalization, we introduce and annotate questions according to three categories that measure different levels and kinds of generalization: training set overlap, compositional generalization (comp-gen), and novel-entity generalization (novel-entity). When evaluating six popular parametric and non-parametric models, we find that for the established Natural Questions and TriviaQA datasets, even the strongest model performance for comp-gen/novel-entity is 13.1/5.4\% and 9.6/1.5\% lower compared to that for the full test set -- indicating the challenge posed by these types of questions. Furthermore, we show that whilst non-parametric models can handle questions containing novel entities relatively well, they struggle with those requiring compositional generalization. Lastly, we find that key question difficulty factors are: cascading errors from the retrieval component, frequency of question pattern, and frequency of the entity.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Liu, Linqing and Lewis, Patrick and Riedel, Sebastian and Stenetorp, Pontus},
	month = may,
	year = {2022},
	note = {arXiv:2109.01156},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{franco_ontology-based_2020,
	address = {Prague, Czech Republic},
	title = {Ontology-based {Question} {Answering} {Systems} over {Knowledge} {Bases}: {A} {Survey}},
	isbn = {9789897584237},
	shorttitle = {Ontology-based {Question} {Answering} {Systems} over {Knowledge} {Bases}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0009392205320539},
	doi = {10.5220/0009392205320539},
	urldate = {2025-07-26},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Enterprise} {Information} {Systems}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Franco, Wellington and Viktor, Caio and Oliveira, Artur and Maia, Gilvan and Brayner, Angelo and Vidal, V. and Carvalho, Fernando and Pequeno, V.},
	year = {2020},
	pages = {532--539},
}

@article{diefenbach_core_2018,
	title = {Core techniques of question answering systems over knowledge bases: a survey},
	volume = {55},
	issn = {0219-1377, 0219-3116},
	shorttitle = {Core techniques of question answering systems over knowledge bases},
	url = {http://link.springer.com/10.1007/s10115-017-1100-y},
	doi = {10.1007/s10115-017-1100-y},
	language = {en},
	number = {3},
	urldate = {2025-07-26},
	journal = {Knowledge and Information Systems},
	author = {Diefenbach, Dennis and Lopez, Vanessa and Singh, Kamal and Maret, Pierre},
	month = jun,
	year = {2018},
	pages = {529--569},
}

@article{antoniou_survey_2022,
	title = {A survey on semantic question answering systems},
	volume = {37},
	issn = {0269-8889, 1469-8005},
	url = {https://www.cambridge.org/core/product/identifier/S0269888921000138/type/journal_article},
	doi = {10.1017/S0269888921000138},
	abstract = {Abstract 
            Recently, many question answering systems that derive answers from linked data repositories have been developed. The purpose of this survey is to identify the common features and approaches of the semantic question answering (SQA) systems, although many different and prototype systems have been designed. The SQA systems use a formal query language like SPARQL and knowledge of a specific vocabulary. This paper analyses different frameworks, architectures, or systems that perform SQA and classifies SQA systems based on different criteria.},
	language = {en},
	urldate = {2025-07-26},
	journal = {The Knowledge Engineering Review},
	author = {Antoniou, Christina and Bassiliades, Nick},
	year = {2022},
	pages = {e2},
}

@misc{mitra_generative_2024,
	title = {A {Generative} {Approach} to {Question} {Answering}},
	url = {https://ar5iv.labs.arxiv.org/html/1711.06238},
	abstract = {Question Answering has come a long way from answer sentence selection, relational QA to reading and comprehension. We shift our attention to generative question answering (gQA) by which we facilitate machine to read pa…},
	language = {en},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Mitra, Rajarshee},
	month = mar,
	year = {2024},
}

@misc{noauthor_streamlit_2025,
	title = {Streamlit {Documentation} v1.47.0},
	url = {https://docs.streamlit.io/},
	urldate = {2025-06-19},
	journal = {Streamlit Inc.},
	month = jul,
	year = {2025},
}

@misc{yousaf_how_2025,
	title = {How {Does} {RAG} {Differ} from {Traditional} {NLP} {Models}?},
	url = {https://dev.to/shaheryaryousaf/how-does-rag-differ-from-traditional-nlp-models-286f},
	abstract = {Artificial Intelligence (AI) has transformed the way computers understand and generate human...},
	language = {en},
	urldate = {2025-07-23},
	journal = {DEV Community},
	author = {Yousaf, Shaheryar},
	month = mar,
	year = {2025},
}

@misc{pavlik_cose_2025,
	title = {Cos'è l'{AI} generativa? {Come} funziona?},
	shorttitle = {Cos'è l'{AI} generativa?},
	url = {https://www.oracle.com/it/artificial-intelligence/generative-ai/what-is-generative-ai/},
	abstract = {Scopri come funziona l'intelligenza artificiale generativa e l'impatto potenzialmente enorme e profondo che può avere sull'economia globale, sul tuo business e sul tuo lavoro.},
	language = {it-IT},
	urldate = {2025-06-19},
	author = {Pavlik, Greg},
	month = feb,
	year = {2025},
}

@incollection{jurafsky_chapter_2024,
	edition = {2},
	title = {Chapter 14: {Question} {Answering}, {Information} {Retrieval}, and {RetrievalAugmented} {Generation}},
	shorttitle = {Speech and {Language} {Processing}},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	urldate = {2025-07-26},
	booktitle = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition} with {Language} {Models}},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2024},
}

@article{caballero_brief_2021,
	title = {A {Brief} {Survey} of {Question} {Answering} {Systems}},
	volume = {12},
	issn = {09762191},
	url = {https://aircconline.com/ijaia/V12N5/12521ijaia01.pdf},
	doi = {10.5121/ijaia.2021.12501},
	abstract = {Question Answering (QA) is a subfield of Natural Language Processing (NLP) and computer science focused on building systems that automatically answer questions from humans in natural language. This survey summarizes the history and current state of the field and is intended as an introductory overview of QA systems. After discussing QA history, this paper summarizes the different approaches to the architecture of QA systems -- whether they are closed or open-domain and whether they are text-based, knowledge-based, or hybrid systems. Lastly, some common datasets in this field are introduced and different evaluation metrics are discussed.},
	number = {5},
	urldate = {2025-07-26},
	journal = {International Journal of Artificial Intelligence \& Applications},
	author = {Caballero, Michael},
	month = sep,
	year = {2021},
	pages = {01--07},
}

@article{boreshban_improving_2023,
	title = {Improving question answering performance using knowledge distillation and active learning},
	volume = {123},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197623003214},
	doi = {10.1016/j.engappai.2023.106137},
	language = {en},
	urldate = {2025-07-26},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Boreshban, Yasaman and Mirbostani, Seyed Morteza and Ghassem-Sani, Gholamreza and Mirroshandel, Seyed Abolghasem and Amiriparian, Shahin},
	month = aug,
	year = {2023},
	pages = {106137},
}

@misc{yang_curiousllm_2025,
	title = {{CuriousLLM}: {Elevating} {Multi}-{Document} {Question} {Answering} with {LLM}-{Enhanced} {Knowledge} {Graph} {Reasoning}},
	shorttitle = {{CuriousLLM}},
	url = {http://arxiv.org/abs/2404.09077},
	doi = {10.48550/arXiv.2404.09077},
	abstract = {Large Language Models (LLMs) have achieved significant success in open-domain question answering. However, they continue to face challenges such as hallucinations and knowledge cutoffs. These issues can be mitigated through in-context learning by providing LLMs with relevant context before generating answers. Recent literature proposes Knowledge Graph Prompting (KGP) which integrates knowledge graphs with an LLM-based traversal agent to substantially enhance document retrieval quality. However, KGP requires costly fine-tuning with large datasets and remains prone to hallucination. In this paper, we propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning mechanism into an LLM agent. This mechanism enables the agent to generate relevant follow-up questions, thereby guiding the information retrieval process more efficiently. Central to our approach is the development of the new Follow-upQA dataset, which includes questions and supporting evidence as input, with follow-up questions serving as ground truths. These follow-up questions either inquire about what is still missing to fully answer the user's query or use special tokens to signify that the retrieved evidence is sufficient. Our experiments show that CuriousLLM significantly boosts LLM performance in multi-document question answering (MD-QA), circumventing the substantial computational costs and latency from the original KGP framework.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Yang, Zukang and Zhu, Zixuan and Zhu, Xuan},
	month = feb,
	year = {2025},
	note = {arXiv:2404.09077},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}


@misc{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	doi = {10.48550/arXiv.1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2025-07-26},
	publisher = {arXiv},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv:1906.08237},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@inproceedings{alqifari_question_2019,
	address = {Varna, Bulgaria},
	title = {Question {Answering} {Systems} {Approaches} and {Challenges}},
	url = {https://aclanthology.org/R19-2011/},
	doi = {10.26615/issn.2603-2821.2019_011},
	abstract = {Question answering (QA) systems permit the user to ask a question using natural language, and the system provides a concise and correct answer. QA systems can be implemented for different types of datasets, structured or unstructured. In this paper, some of the recent studies will be reviewed and the limitations will be discussed. Consequently, the current issues are analyzed with the proposed solutions.},
	urldate = {2025-07-26},
	booktitle = {Proceedings of the {Student} {Research} {Workshop} {Associated} with {RANLP} 2019},
	publisher = {INCOMA Ltd.},
	author = {Alqifari, Reem},
	editor = {Kovatchev, Venelin and Temnikova, Irina and Šandrih, Branislava and Nikolova, Ivelina},
	month = sep,
	year = {2019},
	pages = {69--75},
}

@misc{noauthor_question_2025,
	title = {Question answering},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Question_answering&oldid=1293783575},
	abstract = {Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP) that is concerned with building systems that automatically answer questions that are posed by humans in a natural language.},
	language = {en},
	urldate = {2025-07-26},
	journal = {Wikipedia},
	month = jun,
	year = {2025},
	note = {Page Version ID: 1293783575},
}

@misc{bup_solutions_bup_nodate,
	title = {{BUP} {Solutions}},
	url = {https://www.bupsolutions.com/en/home_en/},
	abstract = {WHERE DATA COMES ALIVE},
	urldate = {2025-07-25},
	journal = {BUP solutions},
	author = {{BUP Solutions}},
}

@article{mcdermott_artificial_1976,
	title = {Artificial intelligence meets natural stupidity},
	issn = {0163-5719},
	url = {https://dl.acm.org/doi/10.1145/1045339.1045340},
	doi = {10.1145/1045339.1045340},
	abstract = {As a field, artificial intelligence has always been on the border of respectability, and therefore on the border of crackpottery. Many critics {\textless}Dreyfus, 1972{\textgreater}, {\textless}Lighthill, 1973{\textgreater} have urged that we are over the border. We have been very defensive toward this charge, drawing ourselves up with dignity when it is made and folding the cloak of Science about us. On the other hand, in private, we have been justifiably proud of our willingness to explore weird ideas, because pursuing them is the only way to make progress.},
	language = {en},
	number = {57},
	urldate = {2025-07-25},
	journal = {ACM SIGART Bulletin},
	author = {McDermott, Drew},
	month = apr,
	year = {1976},
	pages = {4--9},
}

@article{yang_ragva_2025,
	title = {{RAGVA}: {Engineering} retrieval augmented generation-based virtual assistants in practice},
	volume = {226},
	issn = {01641212},
	shorttitle = {{RAGVA}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121225001049},
	doi = {10.1016/j.jss.2025.112436},
	language = {en},
	urldate = {2025-07-23},
	journal = {Journal of Systems and Software},
	author = {Yang, Rui and Fu, Michael and Tantithamthavorn, Chakkrit and Arora, Chetan and Vandenhurk, Lisa and Chua, Joey},
	month = aug,
	year = {2025},
	pages = {112436},
}

@misc{lala_paperqa_2023,
	title = {{PaperQA}: {Retrieval}-{Augmented} {Generative} {Agent} for {Scientific} {Research}},
	shorttitle = {{PaperQA}},
	url = {http://arxiv.org/abs/2312.07559},
	doi = {10.48550/arXiv.2312.07559},
	abstract = {Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Lála, Jakub and O'Donoghue, Odhran and Shtedritski, Aleksandar and Cox, Sam and Rodriques, Samuel G. and White, Andrew D.},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07559},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wang_text_2024,
	title = {Text {Embeddings} by {Weakly}-{Supervised} {Contrastive} {Pre}-training},
	url = {http://arxiv.org/abs/2212.03533},
	doi = {10.48550/arXiv.2212.03533},
	abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
	month = feb,
	year = {2024},
	note = {arXiv:2212.03533},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{gna_wiki_2024,
	title = {Wiki {GNA} {Manuale} {Operativo} v.1.4},
	copyright = {CC BY 4.0},
	url = {https://gna.cultura.gov.it/wiki/index.php/Pagina_principale},
	urldate = {2025-07-24},
	journal = {Geoportale Nazionale per l'Archeologia - Manuale Operativo},
	author = {GNA, MiC},
	month = feb,
	year = {2024},
}

@misc{noauthor_was_nodate,
	title = {Was ist {RAG}? – {Retrieval} {Augmented} {Generation} erklärt – {AWS}},
	shorttitle = {Was ist {RAG}?},
	url = {https://aws.amazon.com/de/what-is/retrieval-augmented-generation/},
	abstract = {Was ist Retrieval-Augmented Generation (RAG), wie und warum verwenden Unternehmen RAG AI und wie verwendet man RAG mit AWS?},
	language = {de-DE},
	urldate = {2025-07-24},
	journal = {Amazon Web Services, Inc.},
}

@misc{noauthor_ada_2025,
	title = {Ada {Gabucci}},
	url = {https://web.archive.org/web/20250724081422/https://conf24.garr.it/it/speaker/ada-gabucci},
	urldate = {2025-07-24},
	month = jul,
	year = {2025},
}

@incollection{acconcia_pubblicazione_2023,
	address = {IT},
	title = {{LA} {PUBBLICAZIONE} {DEI} {DATI} {NEL} {GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	url = {https://doi.org/10.60974/GNA_03},
	language = {ita},
	urldate = {2025-07-24},
	booktitle = {{GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	publisher = {Ministero della Cultura - Istituto Centrale per l'Archeologia},
	author = {Acconcia, Valeria},
	collaborator = {Calandra, Elena and Boi, Valeria and Falcone, Annalisa and Acconcia, Valeria},
	month = jul,
	year = {2023},
	pages = {1},
}

@incollection{gabucci_template_2023,
	address = {IT},
	title = {{UN} {TEMPLATE} {QGIS} {AL} {SERVIZIO} {DEL} {GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	url = {https://doi.org/10.60974/GNA_05},
	language = {ita},
	urldate = {2025-07-24},
	booktitle = {{GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	publisher = {Ministero della Cultura - Istituto Centrale per l'Archeologia},
	author = {Gabucci, Ada},
	collaborator = {Calandra, Elena and Boi, Valeria and Falcone, Annalisa and Acconcia, Valeria},
	month = jul,
	year = {2023},
	pages = {1},
}

@incollection{boi_il_2023,
	address = {IT},
	title = {{IL} {GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}: {STANDARDIZZAZIONE} {E} {APERTURA} {DEI} {DATI}},
	shorttitle = {{IL} {GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	url = {https://doi.org/10.60974/GNA_02},
	language = {ita},
	urldate = {2025-07-24},
	booktitle = {{GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	publisher = {Ministero della Cultura - Istituto Centrale per l'Archeologia},
	author = {Boi, Valeria},
	collaborator = {Calandra, Elena and Boi, Valeria and Falcone, Annalisa and Acconcia, Valeria},
	month = jul,
	year = {2023},
	pages = {1},
}

@incollection{calandra_il_2023,
	address = {IT},
	title = {{IL} {GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA} ({GNA}). {UN}’{INTRODUZIONE}},
	url = {https://doi.org/10.60974/GNA_01},
	language = {ita},
	urldate = {2025-07-24},
	booktitle = {{GEOPORTALE} {NAZIONALE} {PER} {L}’{ARCHEOLOGIA}},
	publisher = {Ministero della Cultura - Istituto Centrale per l'Archeologia},
	author = {Calandra, Elena},
	collaborator = {Calandra, Elena and Boi, Valeria and Falcone, Annalisa and Acconcia, Valeria},
	month = jul,
	year = {2023},
	pages = {1},
}

@misc{mic_mic_2019,
	title = {{MiC} {GNA} {Geoportale} {Nazionale} {Archeologia}},
	url = {https://gna.cultura.gov.it},
	abstract = {MiC GNA Geoportale Nazionale Archeologia},
	language = {it},
	urldate = {2025-07-24},
	journal = {MiC GNA Geoportale Nazionale Archeologia},
	author = {Mic, GNA},
	month = sep,
	year = {2019},
}

@misc{odsc-community_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} ({RAG}): {A} {Synergistic} {Approach} to {NLU} and {NLG}},
	shorttitle = {Retrieval-{Augmented} {Generation} ({RAG})},
	url = {https://opendatascience.com/retrieval-augmented-generation-rag-a-synergistic-approach-to-nlu-and-nlg/},
	abstract = {Editor’s note: Shalvi Mahajan is a speaker for ODSC APAC  on August 13th. Be sure to check out her talk, “Retrieval-Augmented Generation (RAG): A Synergistic Approach to Natural Language Understanding and Generation,” there! Retrieval-Augmented Generation (RAG) represents a significant advancement in Natural Language Processing (NLP) by effectively combining retrieval-based and...},
	language = {en-US},
	urldate = {2025-07-19},
	journal = {Open Data Science - Your News Source for AI, Machine Learning \& more},
	author = {ODSC-Community},
	month = jul,
	year = {2024},
}

@article{alanazi_question_2021,
	title = {Question {Answering} {Systems}: {A} {Systematic} {Literature} {Review}},
	volume = {12},
	issn = {21565570, 2158107X},
	shorttitle = {Question {Answering} {Systems}},
	url = {http://thesai.org/Publications/ViewPaper?Volume=12&Issue=3&Code=IJACSA&SerialNo=59},
	doi = {10.14569/IJACSA.2021.0120359},
	language = {en},
	number = {3},
	urldate = {2025-07-23},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Alanazi, Sarah Saad and Elfadil, Nazar and Jarajreh, Mutsam and Algarni, Saad},
	year = {2021},
}

@misc{jaswal_rag_2025,
	title = {{RAG} vs. {Traditional} {Search}: {A} {Comparative} {Analysis}},
	shorttitle = {{RAG} vs. {Traditional} {Search}},
	url = {https://www.signitysolutions.com/blog/rag-vs-traditional-search-engines},
	abstract = {Enhance your retrieval skills with our practical guide on mastering rag search. Discover effective strategies for optimizing your process—read more now.},
	language = {en},
	urldate = {2025-07-23},
	journal = {Signity Solutions},
	author = {Jaswal, Amrita},
	month = feb,
	year = {2025},
}

@misc{noauthor_rag_2025,
	title = {{RAG} vs {Traditional} {QA}},
	url = {https://www.geeksforgeeks.org/nlp/rag-vs-traditional-qa/},
	abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
	language = {en-US},
	urldate = {2025-07-23},
	journal = {GeeksforGeeks},
	month = apr,
	year = {2025},
}

@inproceedings{ciletti_retrieval-augmented_2025,
	address = {Verona},
	series = {Quaderni di {Umanistica} {Digitale}},
	title = {Retrieval-{Augmented} {Generation} systems for enhanced
access to digital archives},
	isbn = {978-88-942535-9-7},
	url = {https://amsacta.unibo.it/id/eprint/8380/},
	doi = {10.6092/unibo/amsacta/8380},
	language = {en},
	booktitle = {Diversità, {Equità} e {Inclusione}: {Sfide} e {Opportunità} per l’{Informatica} {Umanistica} nell’{Era} dell’{Intelligenza} {Artificiale}, {Proceedings} del {XIV} {Convegno} {Annuale} {AIUCD2025}},
	publisher = {AIUCD},
	author = {Ciletti, Michele},
	month = jun,
	year = {2025},
	pages = {663},
}

@article{hakdagli_hybrid_2024,
	title = {Hybrid {Question}-{Answering} {System}: {A} {FAISS} and {BM25} {Approach} for {Extracting} {Information} from {Technical} {Document}},
	volume = {5},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0},
	issn = {2980-020X},
	shorttitle = {Hybrid {Question}-{Answering} {System}},
	url = {https://journals.orclever.com/oprd/article/view/535},
	doi = {10.56038/oprd.v5i1.535},
	abstract = {In this study, a hybrid question-answering system was developed to accelerate access to information contained in corporate technical documents and to generate appropriate responses to user queries. The system combines dense vector-based retrieval (FAISS) and sparse text-based retrieval (BM25) methods, integrated with the XLM-RoBERTa Large model. Evaluations conducted on a dataset consisting of 23 technical documents demonstrated the system's effectiveness in responding to both semantic and keyword-based queries. This study presents an innovative approach that enables fast and accurate access to information from technical documents, enhancing the efficiency of corporate knowledge management processes.},
	language = {tur},
	number = {1},
	urldate = {2025-07-10},
	journal = {Orclever Proceedings of Research and Development},
	author = {Hakdağlı, Özlem},
	month = dec,
	year = {2024},
	keywords = {FAISS and BM25 integration},
	pages = {226--237},
}

@inproceedings{packowski_optimizing_2024,
	address = {London United Kingdom},
	title = {Optimizing and {Evaluating} {Enterprise} {Retrieval}-{Augmented} {Generation} ({RAG}): {A} {Content} {Design} {Perspective}},
	isbn = {9798400718014},
	shorttitle = {Optimizing and {Evaluating} {Enterprise} {Retrieval}-{Augmented} {Generation} ({RAG})},
	url = {https://dl.acm.org/doi/10.1145/3704137.3704181},
	doi = {10.1145/3704137.3704181},
	language = {en},
	urldate = {2025-07-23},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Advances} in {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Packowski, Sarah and Halilovic, Inge and Schlotfeldt, Jenifer and Smith, Trish},
	month = oct,
	year = {2024},
	pages = {162--167},
}

@misc{zhou_enabling_2025,
	address = {Valencia, Spain},
	type = {conference},
	title = {Enabling interactive {AI} in industry 5.0 with {RAG}-enhanced {GenAI} {Chatbots}},
	url = {https://orca.cardiff.ac.uk/id/eprint/178617/},
	abstract = {Industry 5.0 advances sustainable development through human-machine collaboration and personalised manufacturing. The increase in intelligent industrial equipment creates data scalability challenges for human workers who face difficulties in making decisions based on relevant data sources. Advanced interactive AI systems, capable of integrating diverse data sources and delivering real-time, context-aware insights, present promising solutions to the challenges of the industrial environment. This research introduces a retrieval-augmented generation (RAG)-enhanced Generative artificial intelligence (GenAI) chatbot to address these challenges. The system integrates a variety of information sources, including government reports, news websites, academic studies, and industry reports. This industry 5.0 chatbot aims to offer users extensive knowledge of the industrial sector through a Question-and-Answer interface. It provides relevant and accurate information through intuitive, context-aware interactions to reduce cognitive load for users, which improves decision-making efficiency and user experience. Through experimental evaluation, the RAG-enhanced GenAI chatbot significantly improves accuracy, relevance and user satisfaction, outperforming models like ChatGPT-4o. This system presents an innovative practical solution to tackle Industry 5.0 core issues particularly in enhancing human-machine collaboration and decision-making efficiency. This research contributes to the theoretical and practical development of RAG-enhanced AI systems, laying a foundation for future investigations of industrial AI interaction.},
	language = {en},
	urldate = {2025-07-23},
	author = {Zhou, Tianyu and Wan, Yuwei and Liu, Ying and Kumar, Maneesh},
	year = {2025},
}

@article{vaibhav_retrieval-augmented_2025,
	title = {Retrieval-augmented generation: {The} technical foundation of intelligent {AI} {Chatbots}},
	volume = {26},
	issn = {25819615},
	shorttitle = {Retrieval-augmented generation},
	url = {https://journalwjarr.com/node/1453},
	doi = {10.30574/wjarr.2025.26.1.1571},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as a transformative approach in conversational AI technology, addressing fundamental limitations of traditional chatbot systems. This technical article explores the architecture, mechanisms, and advantages of RAG implementations. Traditional AI chatbots suffer from outdated knowledge bases, hallucination tendencies, and limited context awareness - constraints that RAG effectively overcomes by combining dynamic information retrieval with sophisticated text generation capabilities. The RAG framework operates through a multi-stage process encompassing query processing, information retrieval, contextualization, response generation, and delivery. This hybrid architecture yields substantial improvements in factual accuracy, knowledge recency, system transparency, and operational efficiency. The article further examines critical implementation considerations including vector database selection, embedding model optimization, document chunking strategies, retrieval algorithm configuration, and prompt engineering techniques. Looking toward future developments, the article highlights promising directions including multi-modal capabilities, hybrid retrieval methodologies, adaptive retrieval systems, and enterprise knowledge integration. It demonstrates how RAG represents a significant advancement in creating more intelligent, reliable, and context-aware AI conversational systems.},
	number = {1},
	urldate = {2025-07-23},
	journal = {World Journal of Advanced Research and Reviews},
	author = {Vaibhav, Fanindra Mahajan},
	month = apr,
	year = {2025},
	pages = {4093--4099},
}

@misc{akkiraju_facts_2024,
	title = {{FACTS} {About} {Building} {Retrieval} {Augmented} {Generation}-based {Chatbots}},
	url = {http://arxiv.org/abs/2407.07858},
	doi = {10.48550/arXiv.2407.07858},
	abstract = {Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots."},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Akkiraju, Rama and Xu, Anbang and Bora, Deepak and Yu, Tan and An, Lu and Seth, Vishal and Shukla, Aaditya and Gundecha, Pritam and Mehta, Hridhay and Jha, Ashwin and Raj, Prithvi and Balasubramanian, Abhinav and Maram, Murali and Muthusamy, Guru and Annepally, Shivakesh Reddy and Knowles, Sidney and Du, Min and Burnett, Nick and Javiya, Sean and Marannan, Ashok and Kumari, Mamta and Jha, Surbhi and Dereszenski, Ethan and Chakraborty, Anupam and Ranjan, Subhash and Terfai, Amina and Surya, Anoop and Mercer, Tracey and Thanigachalam, Vinodh Kumar and Bar, Tamar and Krishnan, Sanjana and Kilaru, Samy and Jaksic, Jasmine and Algarici, Nave and Liberman, Jacob and Conway, Joey and Nayyar, Sonu and Boitano, Justin},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07858},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{jiang_towards_2024,
	title = {Towards {Enterprise}-{Specific} {Question}-{Answering} for it {Operations} and {Maintenance} {Based} on {Retrieval}-{Augmented} {Generation} {Mechanism}},
	url = {https://www.ssrn.com/abstract=5069318},
	doi = {10.2139/ssrn.5069318},
	urldate = {2025-07-23},
	author = {Jiang, Zhuoxuan and Zhang, Tianyang and Bai, Shengguang and Lin, Lin and Zhang, Haotian and Xun, Yinong and Ren, Jiawei and Si, Wen and Zhang, Shaohua},
	year = {2024},
}

@misc{carriero_arco_2019,
	title = {{ArCo}: the {Italian} {Cultural} {Heritage} {Knowledge} {Graph}},
	shorttitle = {{ArCo}},
	url = {http://arxiv.org/abs/1905.02840},
	doi = {10.48550/arXiv.1905.02840},
	abstract = {ArCo is the Italian Cultural Heritage knowledge graph, consisting of a network of seven vocabularies and 169 million triples about 820 thousand cultural entities. It is distributed jointly with a SPARQL endpoint, a software for converting catalogue records to RDF, and a rich suite of documentation material (testing, evaluation, how-to, examples, etc.). ArCo is based on the official General Catalogue of the Italian Ministry of Cultural Heritage and Activities (MiBAC) - and its associated encoding regulations - which collects and validates the catalogue records of (ideally) all Italian Cultural Heritage properties (excluding libraries and archives), contributed by CH administrators from all over Italy. We present its structure, design methods and tools, its growing community, and delineate its importance, quality, and impact.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Carriero, Valentina Anita and Gangemi, Aldo and Mancinelli, Maria Letizia and Marinucci, Ludovica and Nuzzolese, Andrea Giovanni and Presutti, Valentina and Veninata, Chiara},
	month = may,
	year = {2019},
	note = {arXiv:1905.02840},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@misc{seo_valuesrag_2025,
	title = {{ValuesRAG}: {Enhancing} {Cultural} {Alignment} {Through} {Retrieval}-{Augmented} {Contextual} {Learning}},
	shorttitle = {{ValuesRAG}},
	url = {http://arxiv.org/abs/2501.01031},
	doi = {10.48550/arXiv.2501.01031},
	abstract = {Ensuring cultural values alignment in Large Language Models (LLMs) remains a critical challenge, as these models often embed Western-centric biases from their training data, leading to misrepresentations and fairness concerns in cross-cultural applications. Existing approaches such as role assignment and few-shot learning struggle to address these limitations effectively due to their reliance on pre-trained knowledge, limited scalability, and inability to capture nuanced cultural values. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with In-Context Learning (ICL) to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. We subsequently curate several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. We evaluate ValuesRAG using 6 diverse regional datasets and show that it consistently outperforms baselines: including zero-shot, role-assignment, few-shot, and hybrid methods, both in main experiments and ablation settings. Notably, ValuesRAG achieves the best overall performance over prior methods, demonstrating its effectiveness in fostering culturally aligned and inclusive AI systems. Our findings underscore the potential of dynamic retrieval-based methods to bridge the gap between global LLM capabilities and localized cultural values.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Seo, Wonduk and Yuan, Zonghao and Bu, Yi},
	month = may,
	year = {2025},
	note = {arXiv:2501.01031},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Social and Information Networks},
}

@misc{agarwal_litllm_2025,
	title = {{LitLLM}: {A} {Toolkit} for {Scientific} {Literature} {Review}},
	shorttitle = {{LitLLM}},
	url = {http://arxiv.org/abs/2402.01788},
	doi = {10.48550/arXiv.2402.01788},
	abstract = {Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-factual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our project page including the demo and toolkit can be accessed here: https://litllm.github.io},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Agarwal, Shubham and Sahu, Gaurav and Puri, Abhay and Laradji, Issam H. and Dvijotham, Krishnamurthy DJ and Stanley, Jason and Charlin, Laurent and Pal, Christopher},
	month = mar,
	year = {2025},
	note = {arXiv:2402.01788},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{debellis_integrating_2024,
	title = {Integrating {Large} {Language} {Models} ({LLM}) and ontologies to {Implement} {Retrieval} {Augmented} {Generation}},
	url = {https://www.michaeldebellis.com/post/integrating-llms-and-ontologies},
	abstract = {Retrieval Augmented Generation is an architecture that integrates ontologies and LLMs and eliminates hallucinations and black-box reasoning},
	language = {en},
	urldate = {2025-07-23},
	journal = {MichaelDeBellis},
	author = {DeBellis, Michael},
	month = jul,
	year = {2024},
}

@inproceedings{xiao_orag_2024,
	title = {{ORAG}: {Ontology}-{Guided} {Retrieval}-{Augmented} {Generation} for {Theme}-{Specific} {Entity} {Typing}},
	shorttitle = {{ORAG}},
	url = {https://openreview.net/forum?id=cKBmZ2PZ6c},
	abstract = {Large language models (LLMs) incorporated with retrieval-augmented generation (RAG) have shown great power in many NLP tasks, including fine-grained entity typing (FET). However, we observe that recent LLMs can easily suffer from hallucinations on highly specialized and fast-evolving themes (e.g., redox-active organic electrode materials), especially in the following cases: (1) unseen entities: an entity never appears in the pre-training corpora of LLMs; and (2) misleading semantics: the context of an entity can potentially mislead an entity typing algorithm if the relevant knowledge is not correctly retrieved and utilized. To address these challenges, this paper proposes an Ontology-Guided Retrieval-Augmented Generation (ORAG) approach that incorporates ontology structures with RAG for the theme-specific entity typing task. ORAG first enriches the label ontology with external knowledge and constructs a structured knowledge unit for each node. Then, it retrieves the relevant nodes by dense passage retrieval and expands the retrieved results based on the ontological structure. In this way, more supporting knowledge will be retrieved within the limited input of LLMs for entity typing. In the evaluation, we construct a dataset with two themes for theme-specific entity typing with a focus on unseen entities and misleading semantics. We observe notable cases of hallucination when vanilla RAG is applied to Llama-3, GPT-3.5, and GPT-4, while ORAG can effectively mitigate such hallucinations and improve the results.},
	language = {en},
	urldate = {2025-07-23},
	author = {Xiao, Jinfeng and Ding, Linyi and Barry, James and Elkaref, Mohab and Mel, Geeth De and Han, Jiawei},
	month = aug,
	year = {2024},
}

@article{bran_ontology-retrieval_2024,
	title = {Ontology-{Retrieval} {Augmented} {Generation} for {Scientific} {Discovery}},
	url = {https://openreview.net/forum?id=DbZDbg2z9q},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, sparkling an increasing interest for their application in science. However, in scientific domains, their utility is often limited by hallucinations that violate established relationships between concepts or ignore their meaning; problems that are not entirely eliminated with Retrieval Augmented Generation (RAG) techniques. A key feature of science is the use of niche concepts, abbreviations and implicit relationships, which may deem RAG approaches less powerful due to the lack of understanding of concepts, especially in emerging and less known fields. Ontologies, as structured frameworks for organizing knowledge and establishing relationships between concepts, offer a potential solution to this challenge. In this work we introduce OntoRAG, a novel approach that enhances RAG by retrieving taxonomical knowledge from ontologies. We evaluate the performance of this method on three common biomedical benchmarks. To extend the value of OntoRAG to emerging fields, where ontologies have not yet been developed, we also present OntoGen, a methodology for generating ontologies from a set of documents. We apply the combined OntoGen+OntoRAG pipeline to a novel benchmark of scientific discovery in the emerging field of single-atom catalysis. Our results demonstrate the promise of this method for improving reasoning and suppressing hallucinations in LLMs, potentially accelerating scientific discovery across various domains.},
	language = {en},
	urldate = {2025-07-23},
	author = {Bran, Andres M. and Oarga, Alexandru and Hart, Matthew and Lederbauer, Magdalena and Schwaller, Philippe},
	month = oct,
	year = {2024},
}

@misc{tiwari_ontorag_2025,
	title = {{OntoRAG}: {Enhancing} {Question}-{Answering} through {Automated} {Ontology} {Derivation} from {Unstructured} {Knowledge} {Bases}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {{OntoRAG}},
	url = {https://arxiv.org/abs/2506.00664},
	doi = {10.48550/ARXIV.2506.00664},
	abstract = {Ontologies are pivotal for structuring knowledge bases to enhance question answering (QA) systems powered by Large Language Models (LLMs). However, traditional ontology creation relies on manual efforts by domain experts, a process that is time intensive, error prone, and impractical for large, dynamic knowledge domains. This paper introduces OntoRAG, an automated pipeline designed to derive ontologies from unstructured knowledge bases, with a focus on electrical relay documents. OntoRAG integrates advanced techniques, including web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation, to transform unstructured data into a queryable ontology. By leveraging LLMs and graph based methods, OntoRAG enhances global sensemaking capabilities, outperforming conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity. Experimental results demonstrate OntoRAGs effectiveness, achieving a comprehensiveness win rate of 85\% against vector RAG and 75\% against GraphRAGs best configuration. This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Tiwari, Yash and Lone, Owais Ahmad and Pal, Mayukha},
	year = {2025},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{park_ontology-based_2024,
	title = {Ontology-based {Retrieval} {Augmented} {Generation} ({RAG}) for {GenAI}-supported {Additive} {Manufacturing}},
	url = {https://www.nist.gov/publications/ontology-based-retrieval-augmented-generation-rag-genai-supported-additive},
	abstract = {Conventional data analytics often fail to capture the intricate context of Additive Manufacturing (AM) processes, leading to pointed solutions and suboptimal an},
	language = {en},
	urldate = {2025-07-23},
	journal = {NIST},
	author = {Park, Yeun and Witherell, Paul and Surovi, Nowrin Akter and Cho, Hyunbo},
	month = aug,
	year = {2024},
}

@article{ludwig_ontology-based_2025,
	title = {An ontology-based retrieval augmented generation procedure for a voice-controlled maintenance assistant},
	volume = {169},
	issn = {01663615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166361525000545},
	doi = {10.1016/j.compind.2025.104289},
	language = {en},
	urldate = {2025-07-23},
	journal = {Computers in Industry},
	author = {Ludwig, Heiner and Schmidt, Thorsten and Kühn, Mathias},
	month = aug,
	year = {2025},
	pages = {104289},
}

@misc{belagatti_enhance_2024,
	title = {Enhance {Your} {RAG} {Applications} with {Knowledge} {Graph} {RAG} {\textbar} {Build} {Intelligent} {Apps} {With} {SingleStore}},
	url = {https://www.singlestore.com/blog/enhance-your-rag-applications-with-knowledge-graph-rag/},
	abstract = {Dive deeper into understanding how a knowledge graph approach helps RAG applications.},
	language = {en},
	urldate = {2025-07-23},
	journal = {SingleStore},
	author = {Belagatti, Pavan and Bhamidipati, Rohit},
	month = oct,
	year = {2024},
}

@misc{sharma_og-rag_2024,
	title = {{OG}-{RAG}: {Ontology}-{Grounded} {Retrieval}-{Augmented} {Generation} {For} {Large} {Language} {Models}},
	shorttitle = {{OG}-{RAG}},
	url = {http://arxiv.org/abs/2412.15235},
	doi = {10.48550/arXiv.2412.15235},
	abstract = {This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented Generation method designed to enhance LLM-generated responses by anchoring retrieval processes in domain-specific ontologies. While LLMs are widely used for tasks like question answering and search, they struggle to adapt to specialized knowledge, such as industrial workflows or knowledge work, without expensive fine-tuning or sub-optimal retrieval methods. Existing retrieval-augmented models, such as RAG, offer improvements but fail to account for structured domain knowledge, leading to suboptimal context generation. Ontologies, which conceptually organize domain knowledge by defining entities and their interrelationships, offer a structured representation to address this gap. OG-RAG constructs a hypergraph representation of domain documents, where each hyperedge encapsulates clusters of factual knowledge grounded using domain-specific ontology. An optimization algorithm then retrieves the minimal set of hyperedges that constructs a precise, conceptually grounded context for the LLM. This method enables efficient retrieval while preserving the complex relationships between entities. OG-RAG applies to domains where fact-based reasoning is essential, particularly in tasks that require workflows or decision-making steps to follow predefined rules and procedures. These include industrial workflows in healthcare, legal, and agricultural sectors, as well as knowledge-driven tasks such as news journalism, investigative research, consulting and more. Our evaluations demonstrate that OG-RAG increases the recall of accurate facts by 55\% and improves response correctness by 40\% across four different LLMs. Additionally, OG-RAG enables 30\% faster attribution of responses to context and boosts fact-based reasoning accuracy by 27\% compared to baseline methods.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Sharma, Kartik and Kumar, Peeyush and Li, Yunqing},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15235},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{barbato_nasce_2025,
	title = {Nasce {Cat}-{IA}, l’agente conversazionale che semplifica la consultazione del {Catalogo} generale dei beni culturali},
	url = {https://digitallibrary.cultura.gov.it/notizie/nasce-cat-ia/},
	abstract = {Da oggi disponibile sul portale del Catalogo generale dei beni culturali la versione sperimentale di Cat-IA},
	language = {it-IT},
	urldate = {2025-07-21},
	journal = {Istituto Centrale per la Digitalizzazione del Patrimonio Culturale},
	author = {Barbato, Fabrizio},
	month = apr,
	year = {2025},
}

@inproceedings{salemi_evaluating_2024,
	address = {Washington DC USA},
	title = {Evaluating {Retrieval} {Quality} in {Retrieval}-{Augmented} {Generation}},
	isbn = {9798400704314},
	url = {https://dl.acm.org/doi/10.1145/3626772.3657957},
	doi = {10.1145/3626772.3657957},
	language = {en},
	urldate = {2025-07-21},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Salemi, Alireza and Zamani, Hamed},
	month = jul,
	year = {2024},
	pages = {2395--2400},
}

@misc{pograri_question-answering_2025,
	title = {Question-{Answering} {AI} {Assistant} for {Geoportale} {Nazionale} {Archeologia} ({GNA})},
	url = {https://zenodo.org/records/16259759},
	abstract = {Initial release of the GNA QA system. Includes web crawling, document chunking, vector-based retrieval, citation-aware generation, Streamlit UI, and evaluation pipeline.},
	urldate = {2025-07-21},
	publisher = {Zenodo},
	author = {Pograri, Lucrezia},
	month = jul,
	year = {2025},
	doi = {10.5281/zenodo.16259759},
	keywords = {Digital humanities},
}

@mastersthesis{nicoletti_llms_2025,
	title = {{LLMs} and {Essence}: {Developing} a {Chatbot} to {Support} {Software} {Engineering} {Practices}},
	copyright = {studio\_ricerca},
	shorttitle = {{LLMs} and {Essence}},
	url = {https://amslaurea.unibo.it/id/eprint/34197/},
	abstract = {Recent advancements in natural language processing (NLP) have led to the development of various applications in a wide range of domains. In the field of software engineering, NLP research has extensively focused on areas like code generation while overlooking other aspects such as project management and best practice recommendations. This thesis aims to fill this gap by investigating the integration of Essence, a standard and thinking framework designed to support the management of software engineering practices, and large language models (LLMs), one of the latest NLP techniques. To achieve this, a specialised chatbot was developed as a complementary tool to assist students and professionals in learning about Essence and improving their software engineering processes. The chatbot utilises a retrieval-augmented generation system to retrieve relevant context from a curated knowledge base, augmenting the users’ queries to generate more accurate and context-aware responses. Various optimisation strategies were implemented to enhance the system’s performance, which was evaluated using metrics assessing both the relevance of retrieved context and the quality of generated responses. Comparative analysis with a general-purpose LLM demonstrated that the proposed system consistently outperforms its counterpart in domain-specific tasks. Although further testing with real users is required to fully understand the application’s potential, this work establishes a foundation for future research into improving the learning and adoption of software engineering practices.},
	language = {en},
	urldate = {2025-07-21},
	school = {Computer Science, University of Bologna},
	author = {Nicoletti, Sonia},
	month = feb,
	year = {2025},
}

@mastersthesis{caramanna_progettazione_2024,
	title = {Progettazione e sviluppo di un chatbot basato su tecniche di {Intelligenza} {Artificiale} {Generativa}},
	copyright = {studio\_ricerca},
	url = {https://amslaurea.unibo.it/id/eprint/32820/},
	abstract = {L’obiettivo principale di questa tesi è mettere in mostra come le tecniche avanzate di Recupero Generativo Aumentato (RAG, Retrieval-Augmented Generation) possano migliorare l’efficienza
e le prestazioni di Cesare, un chatbot virtuale sviluppato per un’azienda il cui scopo è quello di rispondere in maniera precisa alle domande dell’utente su tematiche aziendali. Oltre
ad illustrare in maniera approfondita sia il processo di una RAG semplice (NAIVE) sia quali potrebbero essere le tecniche di RAG avanzate, come per esempio “Metadata Addiction”, “Hypothetical
questions” oppure “Embedding models fine-tuning”, vengono discusse e analizzate tre tecniche ibride create ex novo per migliorare ulteriormente il comportamento del chatbot.
I risultati ottenuti grazie all’utilizzo della libreria “RAGAs” mostrano che, nonostante le loro differenze concettuali, queste tre tecniche si eguagliano in termini di prestazioni, fornendo tutte
risultati ottimali e comparabili.},
	language = {it},
	urldate = {2025-07-21},
	school = {Ingegneria Informatica, Università di Bologna},
	author = {Caramanna, Gianluigi},
	month = oct,
	year = {2024},
}

@mastersthesis{florio_progettazione_2024,
	title = {Progettazione e implementazione di un chatbot intelligente tramite piattaforma {LangChain}: studio e valutazione dei {VectorDB}},
	copyright = {cc\_by\_nc\_nd4},
	shorttitle = {Progettazione e implementazione di un chatbot intelligente tramite piattaforma {LangChain}},
	url = {https://amslaurea.unibo.it/id/eprint/32282/},
	abstract = {In questo elaborato di tesi sarà trattato il progetto che consiste nella progettazione e nello sviluppo di un chatbot basato su un RAG, implementato mediante l'utilizzo della piattaforma Langchain, per conto dell'azienda Data Reply srl. Innanzitutto, saranno presentati argomenti come Generative AI, LLM e LangChain, sarà fornita una definizione del concetto di RAG, sarà esplicitata la pipeline mediante la quale si è giunti al chatbot finale e saranno descritte nel dettaglio le API utilizzate. In secondo luogo, sarà trattata la tematica relativa ai vectorDB, analizzandone la natura, il funzionamento e le prestazioni derivanti da ognuno di essi.},
	language = {it},
	urldate = {2025-07-21},
	author = {Florio, Michelangelo},
	month = aug,
	year = {2024},
}

@mastersthesis{salcuni_utilizzo_2025,
	title = {Utilizzo di tecniche {RAG} per la {Valutazione} e {Comparazione} dei {Modelli} {LLM} in ambito medico},
	copyright = {studio\_ricerca},
	url = {https://amslaurea.unibo.it/id/eprint/33387/},
	abstract = {La presente tesi esplora l'applicazione della tecnica Retrieval-Augmented Generation (RAG) combinata con diversi modelli di linguaggio di grandi dimensioni (LLM), con particolare attenzione all’ambito medico. L'obiettivo principale è analizzare come la tecnica RAG possa migliorare la qualità delle risposte generate, in particolare nell'identificazione di pattern clinici e nella personalizzazione delle strategie terapeutiche. Viene fornita una panoramica sull'evoluzione dei LLM, con un focus sulle applicazioni in ambito sanitario. Inoltre, la tesi valuta le performance dei modelli tramite la libreria RAGAS, analizzando metriche quali accuratezza, robustezza e fedeltà delle risposte in contesti clinici. Il lavoro culmina in un caso studio sulla gestione dell'ipertensione, utilizzando la tecnica RAG per migliorare la rilevazione e il monitoraggio continuo della pressione arteriosa. Si esplorano le potenzialità dell'IA nel superare le limitazioni dei metodi diagnostici tradizionali e nel supportare la personalizzazione delle terapie. I risultati evidenziano il potenziale dei LLM specializzati nell’ottimizzazione della gestione dei pazienti ipertesi, aprendo la strada a future applicazioni di IA nel settore sanitario, con impatti positivi sulla qualità della cura e l'efficienza operativa.},
	language = {it},
	urldate = {2025-07-21},
	school = {Ingegneria Informatica, Università di Bologna},
	author = {Salcuni, Giuseppe Pio},
	month = feb,
	year = {2025},
}

@misc{alshammari_knimezobot_2023,
	title = {{KNIMEZoBot}: {Enhancing} {Literature} {Review} with {Zotero} and {KNIME} {OpenAI} {Integration} using {Retrieval}-{Augmented} {Generation}},
	shorttitle = {{KNIMEZoBot}},
	url = {http://arxiv.org/abs/2311.04310},
	doi = {10.48550/arXiv.2311.04310},
	abstract = {Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.},
	urldate = {2025-07-21},
	publisher = {arXiv},
	author = {Alshammari, Suad and Basalelah, Lama and Rukbah, Walaa Abu and Alsuhibani, Ali and Wijesinghe, Dayanjan S.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04310},
	keywords = {Computer Science - Human-Computer Interaction},
}

@mastersthesis{antolini_experimental_2025,
	title = {Experimental {Study} on {Retrieval}-{Augmented} {Generation}: {Engineering} and {Evaluation} of a {Custom} {RAG} system for {Open}-{Domain} {QA}},
	shorttitle = {Experimental {Study} on {Retrieval}-{Augmented} {Generation}},
	url = {https://thesis.unipd.it/handle/20.500.12608/86949},
	abstract = {open},
	language = {en},
	urldate = {2025-07-21},
	school = {Computer Engineering, University of Padova},
	author = {Antolini, Gianluca},
	month = jul,
	year = {2025},
}

@misc{aytar_retrieval-augmented_2024,
	title = {A {Retrieval}-{Augmented} {Generation} {Framework} for {Academic} {Literature} {Navigation} in {Data} {Science}},
	url = {http://arxiv.org/abs/2412.15404},
	doi = {10.48550/arXiv.2412.15404},
	abstract = {In the rapidly evolving field of data science, efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation. This paper presents an enhanced Retrieval-Augmented Generation (RAG) application, an artificial intelligence (AI)-based system designed to assist data scientists in accessing precise and contextually relevant academic resources. The AI-powered application integrates advanced techniques, including the GeneRation Of BIbliographic Data (GROBID) technique for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method, to significantly improve the relevance and accuracy of the retrieved information. This implementation of AI specifically addresses the challenge of academic literature navigation. A comprehensive evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS) framework demonstrates substantial improvements in key metrics, particularly Context Relevance, underscoring the system's effectiveness in reducing information overload and enhancing decision-making processes. Our findings highlight the potential of this enhanced Retrieval-Augmented Generation system to transform academic exploration within data science, ultimately advancing the workflow of research and innovation in the field.},
	urldate = {2025-07-21},
	publisher = {arXiv},
	author = {Aytar, Ahmet Yasin and Kilic, Kemal and Kaya, Kamer},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15404},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@misc{callaghan_prototyping_2025,
	title = {Prototyping a {RAG} {System} for {Digital} {Humanities}: {Exploring} {AI}/{ML} with {Indigenous} {Data} {\textbar} {King}'s {Digital} {Lab}},
	shorttitle = {Prototyping a {RAG} {System} for {Digital} {Humanities}},
	url = {https://kdl.kcl.ac.uk/blog/ireal-rag/},
	abstract = {A prototype RAG system for Indigenous data, exploring the use of AI/ML in the context of sensitive cultural data.},
	language = {en},
	urldate = {2025-07-19},
	author = {Callaghan, Samantha and Vieira, Miguel},
	month = jan,
	year = {2025},
}

@misc{martineau_what_2023,
	title = {What is retrieval-augmented generation ({RAG})?},
	copyright = {© Copyright IBM Corp. 2021},
	url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
	abstract = {RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI’s decision making process.},
	language = {en-US},
	urldate = {2025-07-19},
	journal = {IBM Research},
	author = {Martineau, Kim},
	year = {2023},
}

@article{han_automating_2024,
	title = {Automating {Systematic} {Literature} {Reviews} with {Retrieval}-{Augmented} {Generation}: {A} {Comprehensive} {Overview}},
	volume = {14},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {Automating {Systematic} {Literature} {Reviews} with {Retrieval}-{Augmented} {Generation}},
	url = {https://www.mdpi.com/2076-3417/14/19/9103},
	doi = {10.3390/app14199103},
	abstract = {This study examines Retrieval-Augmented Generation (RAG) in large language models (LLMs) and their significant application for undertaking systematic literature reviews (SLRs). RAG-based LLMs can potentially automate tasks like data extraction, summarization, and trend identification. However, while LLMs are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. RAG mitigates these limitations by integrating LLMs’ generative capabilities with the precision of real-time information retrieval. We review in detail the three key processes of the RAG framework—retrieval, augmentation, and generation. We then discuss applications of RAG-based LLMs to SLR automation and highlight future research topics, including integration of domain-specific LLMs, multimodal data processing and generation, and utilization of multiple retrieval sources. We propose a framework of RAG-based LLMs for automating SRLs, which covers four stages of SLR process: literature search, literature screening, data extraction, and information synthesis. Future research aims to optimize the interaction between LLM selection, training strategies, RAG techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends.},
	language = {en},
	number = {19},
	urldate = {2025-07-19},
	journal = {Applied Sciences},
	author = {Han, Binglan and Susnjak, Teo and Mathrani, Anuradha},
	month = oct,
	year = {2024},
	pages = {9103},
}

@article{bevara_prospects_2025,
	title = {Prospects of {Retrieval} {Augmented} {Generation} ({RAG}) for {Academic} {Library} {Search} and {Retrieval}},
	volume = {44},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0},
	issn = {2163-5226, 0730-9295},
	url = {https://ital.corejournals.org/index.php/ital/article/view/17361},
	doi = {10.5860/ital.v44i2.17361},
	abstract = {This paper examines the integration of retrieval-augmented generation (RAG) systems within academic library environments, focusing on their potential to transform traditional search and retrieval mechanisms. RAG combines the natural language understanding capabilities of large language models with structured retrieval from verified knowledge bases, offering a novel approach to academic information discovery. The study analyzes the technical requirements for implementing RAG in library systems, including embedding pipelines, vector databases, and middleware architecture for integration with existing library infrastructure. We explore how RAG systems can enhance search precision through semantic indexing, real-time query processing, and contextual understanding while maintaining compliance with data privacy and copyright regulations. The research highlights RAG’s ability to improve user experience through personalized research assistance, conversational interfaces, and multimodal content integration. Critical considerations including ethical implications, copyright compliance, and system transparency are addressed. Our findings indicate that while RAG presents significant opportunities for advancing academic library services, successful implementation requires careful attention to technical architecture, data protection, and user trust. The study concludes that RAG integration holds promise for revolutionizing academic library services while emphasizing the need for continued research in areas of scalability, ethical compliance, and cost-effective implementation.},
	number = {2},
	urldate = {2025-07-19},
	journal = {Information Technology and Libraries},
	author = {Bevara, Ravi Varma Kumar and Lund, Brady D. and Mannuru, Nishith Reddy and Karedla, Sai Pranathi and Mohammed, Yara and Kolapudi, Sai Tulasi and Mannuru, Aashrith},
	month = jun,
	year = {2025},
}

@article{davis_unlocking_2025,
	title = {Unlocking web archives: {LLMs}, {RAG}, and the future of digital preservation},
	shorttitle = {Unlocking web archives},
	url = {https://hdl.handle.net/1828/21379},
	abstract = {Large Language Models (LLMs) are transforming how research libraries manage digital preservation and access to web archives. This paper examines the potential and challenges of integrating LLMs with Retrieval-Augmented Generation (RAG) to enhance the searchability and usability of Web ARChive (WARC) files. Traditional keyword-based retrieval often falls short in handling the complexity of web archives, necessitating new AI-driven approaches. The study explores WARC-GPT, an open-source tool developed by the Harvard Law Library Innovation Lab, which applies RAG techniques to enable conversational search across web archives. While WARC-GPT demonstrates promise, it also encounters significant hurdles, including noisy data, hallucinations, and computational inefficiencies. To address these issues, the author develops a bespoke RAG pipeline optimized for research library needs, implementing improvements in data preprocessing, chunking strategies, and hardware acceleration. The results highlight the potential for AI-enhanced discovery while underscoring the technical, ethical, and resource-related challenges that libraries must navigate. This paper argues that while AI-driven tools offer new avenues for digital preservation, their successful deployment requires careful design, iterative refinement, and human oversight. The future of AI in research libraries will not replace human expertise but will instead rely on a balanced interplay between automation and curation.},
	language = {en},
	urldate = {2025-07-19},
	author = {Davis, Corey},
	month = feb,
	year = {2025},
}

@inproceedings{ramos-varela_context_2025,
	address = {Bilbao, Spain},
	title = {Context or {Retrieval}? {Evaluating} {RAG} {Methods} for {Art} and {Museum} {QA} {System}},
	isbn = {9798891762480},
	shorttitle = {Context or {Retrieval}?},
	url = {https://aclanthology.org/2025.iwsds-1.10/},
	abstract = {Recent studies suggest that increasing the context window of language models could outperform retrieval-augmented generation (RAG) methods in certain tasks. However, in domains such as art and museums, where information is inherently multimodal, combining images and detailed textual descriptions, this assumption needs closer examination. To explore this, we compare RAG techniques with direct large-context input approaches for answering questions about artworks. Using a dataset of painting images paired with textual information, we develop a synthetic database of question-answer (QA) pairs for evaluating these methods. The focus is on assessing the efficiency and accuracy of RAG in retrieving and using relevant information compared to passing the entire textual context to a language model. Additionally, we experiment with various strategies for segmenting and retrieving text to optimise the RAG pipeline. The results aim to clarify the trade-offs between these approaches and provide valuable insights for interactive systems designed for art and museum contexts.},
	urldate = {2025-07-19},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Spoken} {Dialogue} {Systems} {Technology}},
	publisher = {Association for Computational Linguistics},
	author = {Ramos-Varela, Samuel and Bellver-Soler, Jaime and Estecha-Garitagoitia, Marcos and D'Haro, Luis Fernando},
	editor = {Torres, Maria Ines and Matsuda, Yuki and Callejas, Zoraida and del Pozo, Arantza and D'Haro, Luis Fernando},
	month = may,
	year = {2025},
	pages = {129--136},
}

@misc{soman_observations_2024,
	title = {Observations on {Building} {RAG} {Systems} for {Technical} {Documents}},
	url = {http://arxiv.org/abs/2404.00657},
	doi = {10.48550/arXiv.2404.00657},
	abstract = {Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Soman, Sumit and Roychowdhury, Sujoy},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00657},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{fan_research_2025,
	title = {Research on {Graph}-{Retrieval} {Augmented} {Generation} {Based} on {Historical} {Text} {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2506.15241},
	doi = {10.48550/arXiv.2506.15241},
	abstract = {This article addresses domain knowledge gaps in general large language models for historical text analysis in the context of computational humanities and AIGC technology. We propose the Graph RAG framework, combining chain-of-thought prompting, self-instruction generation, and process supervision to create a The First Four Histories character relationship dataset with minimal manual annotation. This dataset supports automated historical knowledge extraction, reducing labor costs. In the graph-augmented generation phase, we introduce a collaborative mechanism between knowledge graphs and retrieval-augmented generation, improving the alignment of general models with historical knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B, with Simplified Chinese input and chain-of-thought prompting, achieves optimal performance in relation extraction (F1 = 0.68). The DeepSeek model integrated with GraphRAG improves F1 by 11\% (0.08-0.19) on the open-domain C-CLUE relation extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12), effectively alleviating hallucinations phenomenon, and improving interpretability. This framework offers a low-resource solution for classical text knowledge extraction, advancing historical knowledge services and humanities research.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Fan, Yang and Qi, Zhang and Wenqian, Xing and Chang, Liu and Liu, Liu},
	month = jun,
	year = {2025},
	note = {arXiv:2506.15241},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_retrieval-augmented_2025,
	title = {Retrieval-augmented generation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Retrieval-augmented_generation&oldid=1300820373},
	abstract = {Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.
RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, "RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts." This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.
RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.
The term RAG was first introduced in a 2020 research paper from Meta.},
	language = {en},
	urldate = {2025-07-19},
	journal = {Wikipedia},
	month = jul,
	year = {2025},
	note = {Page Version ID: 1300820373},
}

@misc{sergeev_talking_2025,
	title = {Talking to {Data}: {Designing} {Smart} {Assistants} for {Humanities} {Databases}},
	shorttitle = {Talking to {Data}},
	url = {http://arxiv.org/abs/2506.00986},
	doi = {10.48550/arXiv.2506.00986},
	abstract = {Access to humanities research databases is often hindered by the limitations of traditional interaction formats, particularly in the methods of searching and response generation. This study introduces an LLM-based smart assistant designed to facilitate natural language communication with digital humanities data. The assistant, developed in a chatbot format, leverages the RAG approach and integrates state-of-the-art technologies such as hybrid search, automatic query generation, text-to-SQL filtering, semantic database search, and hyperlink insertion. To evaluate the effectiveness of the system, experiments were conducted to assess the response quality of various language models. The testing was based on the Prozhito digital archive, which contains diary entries from predominantly Russian-speaking individuals who lived in the 20th century. The chatbot is tailored to support anthropology and history researchers, as well as non-specialist users with an interest in the field, without requiring prior technical training. By enabling researchers to query complex databases with natural language, this tool aims to enhance accessibility and efficiency in humanities research. The study highlights the potential of Large Language Models to transform the way researchers and the public interact with digital archives, making them more intuitive and inclusive. Additional materials are presented in GitHub repository: https://github.com/alekosus/talking-to-data-intersys2025.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Sergeev, Alexander and Goloviznina, Valeriya and Melnichenko, Mikhail and Kotelnikov, Evgeny},
	month = jun,
	year = {2025},
	note = {arXiv:2506.00986},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_searching_2024,
	title = {Searching for {Best} {Practices} in {Retrieval}-{Augmented} {Generation}},
	url = {http://arxiv.org/abs/2407.01219},
	doi = {10.48550/arXiv.2407.01219},
	abstract = {Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a "retrieval as generation" strategy.},
	urldate = {2025-07-18},
	publisher = {arXiv},
	author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01219},
	keywords = {Computer Science - Computation and Language},
}

@misc{skarlinski_language_2024,
	title = {Language agents achieve superhuman synthesis of scientific knowledge},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2409.13740},
	doi = {10.48550/ARXIV.2409.13740},
	abstract = {Language models are known to hallucinate incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of which 70\% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Skarlinski, Michael D. and Cox, Sam and Laurent, Jon M. and Braza, James D. and Hinks, Michaela and Hammerling, Michael J. and Ponnapati, Manvitha and Rodriques, Samuel G. and White, Andrew D.},
	year = {2024},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Physical sciences, Information Retrieval (cs.IR), PaperQA, Physics and Society (physics.soc-ph)},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2025-07-10},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	year = {2024},
	note = {arXiv:2312.10997},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@techreport{bor-woei_generative_2024,
	address = {Grenoble, France},
	title = {Generative {Large} {Language} {Models} {Augmented} {Hybrid}
Retrieval {System} for {Biomedical} {Question} {Answering}},
	url = {https://ceur-ws.org/Vol-3740/paper-12.pdf},
	institution = {University of Padova, Italy},
	author = {Bor-Woei, Huang},
	year = {2024},
}

@misc{mulani_improve_2024,
	title = {Improve {Retrieval} {Augmented} {Generation} ({RAG}) with {Re}-ranking},
	url = {https://medium.com/@ashpaklmulani/improve-retrieval-augmented-generation-rag-with-re-ranking-31799c670f8e},
	abstract = {In the world of GenAI, you’ll often come across the term RAG (Retrieval augmented Generation). Essentially, RAG is about giving additional…},
	language = {en},
	urldate = {2025-06-19},
	journal = {Medium},
	author = {Mulani, Ashpak},
	year = {2024},
}

@misc{espindola_chunk_2024,
	title = {Chunk {Division} and {Overlap}: {Understanding} the {Process}},
	shorttitle = {Chunk {Division} and {Overlap}},
	url = {https://gustavo-espindola.medium.com/chunk-division-and-overlap-understanding-the-process-ade7eae1b2bd},
	abstract = {Diving into the world of text and document processing, “chunking” is a fundamental technique. But what does it mean? Essentially, it’s the…},
	language = {en},
	urldate = {2025-06-19},
	journal = {Medium},
	author = {Espíndola, Gustavo},
	year = {2024},
}

@misc{glantz_12_2024,
	title = {12 {RAG} {Pain} {Points} and {Proposed} {Solutions}},
	url = {https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c/},
	abstract = {Solving the core challenges of Retrieval-Augmented Generation},
	language = {en-US},
	urldate = {2025-06-19},
	journal = {Towards Data Science},
	author = {Glantz, Wenqi},
	year = {2024},
}

@article{harsh_comprehending_2024,
	title = {Comprehending and {Reducing} {LLM} {Hallucinations}},
	issn = {2456-2165},
	url = {https://www.ijisrt.com/comprehending-and-reducing-llm-hallucinations},
	doi = {10.38124/ijisrt/IJISRT24JUL882},
	abstract = {The integration of large language models (LLM) into many artificial intelligence applications shows the best performance in tasks such as text mining, typing, question answering. Despite his success, his LL.M. The biggest concern is the emergence of so-called "hallucinations", especially in text-based systems and Q\&As that rely on LL M. These hearings may lead to the spread of misinformation or fraud. This article explains the basics of AI illusions and highlights their importance in AI. Work involves deploying visualizations to a variety of tasks, including machine translation, surveys, interviews, content writing, LLM maps, and visualization questions. Additionally, this article explores potential strategies to reduce negative perceptions in order to increase the overall credibility of the LL.M.},
	language = {en},
	urldate = {2025-06-19},
	journal = {International Journal of Innovative Science and Research Technology (IJISRT)},
	author = {Harsh and Shobha, T.},
	year = {2024},
	pages = {1222--1227},
}

@misc{farenas_fabian_2024,
	title = {Fabian. {Figuring} {Out} the {Ideal} {Chunk} {Size}},
	url = {https://medium.com/@farenas1/fabian-7d1f90ac4cb4},
	abstract = {Figuring Out the Ideal Chunk Size},
	language = {en},
	urldate = {2025-06-19},
	journal = {Medium},
	author = {{Farenas}},
	year = {2024},
}

@article{abu_shawar_chatbots_2007,
	title = {Chatbots: {Are} they {Really} {Useful}?},
	volume = {22},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2190-6858},
	shorttitle = {Chatbots},
	url = {https://jlcl.org/article/view/88},
	doi = {10.21248/jlcl.22.2007.88},
	number = {1},
	urldate = {2025-06-19},
	journal = {Journal for Language Technology and Computational Linguistics},
	author = {Abu Shawar, Bayan and Atwell, Eric},
	year = {2007},
	pages = {29--49},
}

@misc{gupta_comprehensive_2024,
	title = {A {Comprehensive} {Survey} of {Retrieval}-{Augmented} {Generation} ({RAG}): {Evolution}, {Current} {Landscape} and {Future} {Directions}},
	shorttitle = {A {Comprehensive} {Survey} of {Retrieval}-{Augmented} {Generation} ({RAG})},
	url = {http://arxiv.org/abs/2410.12837},
	doi = {10.48550/arXiv.2410.12837},
	abstract = {This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Gupta, Shailja and Ranjan, Rajesh and Singh, Surya Narayan},
	year = {2024},
	note = {arXiv:2410.12837},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{mishra_using_2024,
	title = {Using {LangChain} for {Question} {Answering} on own data},
	url = {https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed},
	abstract = {Step-by-step guide to using langchain to chat with own data},
	language = {en},
	urldate = {2025-06-19},
	journal = {Medium},
	author = {Mishra, Onkar},
	year = {2024},
}

@misc{liu_lost_2023,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Lost in the {Middle}},
	url = {https://arxiv.org/abs/2307.03172},
	doi = {10.48550/ARXIV.2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	year = {2023},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.18223},
	doi = {10.48550/ARXIV.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	year = {2023},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/ARXIV.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{es_ragas_2023,
	title = {Ragas: {Automated} {Evaluation} of {Retrieval} {Augmented} {Generation}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Ragas},
	url = {https://arxiv.org/abs/2309.15217},
	doi = {10.48550/ARXIV.2309.15217},
	abstract = {We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions {\textbackslash}textit\{without having to rely on ground truth human annotations\}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
	year = {2023},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{arslan_survey_2024,
	title = {A {Survey} on {RAG} with {LLMs}},
	volume = {246},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050924021860},
	doi = {10.1016/j.procs.2024.09.178},
	language = {en},
	urldate = {2025-06-19},
	journal = {Procedia Computer Science},
	author = {Arslan, Muhammad and Ghanem, Hussam and Munawar, Saba and Cruz, Christophe},
	year = {2024},
	pages = {3781--3790},
}

@misc{lewis_retrieval-augmented_2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2005.11401},
	doi = {10.48550/ARXIV.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	year = {2020},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{lumer_graph_2025,
	title = {Graph {RAG}-{Tool} {Fusion}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2502.07223},
	doi = {10.48550/ARXIV.2502.07223},
	abstract = {Recent developments in retrieval-augmented generation (RAG) for selecting relevant tools from a tool knowledge base enable LLM agents to scale their complex tool calling capabilities to hundreds or thousands of external tools, APIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails to capture structured dependencies between tools, limiting the retrieval accuracy of a retrieved tool's dependencies. For example, among a vector database of tools, a "get stock price" API requires a "stock ticker" parameter from a "get stock ticker" API, and both depend on OS-level internet connectivity tools. In this paper, we address this limitation by introducing Graph RAG-Tool Fusion, a novel plug-and-play approach that combines the strengths of vector-based retrieval with efficient graph traversal to capture all relevant tools (nodes) along with any nested dependencies (edges) within the predefined tool knowledge graph. We also present ToolLinkOS, a new tool selection benchmark of 573 fictional tools, spanning over 15 industries, each with an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool Fusion achieves absolute improvements of 71.7\% and 22.1\% over naïve RAG on ToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS dataset is available at https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS},
	urldate = {2025-06-19},
	publisher = {arXiv},
	author = {Lumer, Elias and Basavaraju, Pradeep Honaganahalli and Mason, Myles and Burke, James A. and Subbiah, Vamse Kumar},
	year = {2025},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}
