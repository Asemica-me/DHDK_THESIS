\chapter{Methodology}
\label{chap:methodology}
\begin{spacing}{1.5}
This chapter details the methodological workflow for designing and implementing the GNA QA system. The system leverages a RAG pipeline tailored to the Geoportale Nazionale per l’Archeologia (GNA) knowledge base (KB). It comprises modular components for data acquisition, preprocessing, retrieval, generation, feedback collection, and evaluation. The methodology evolved through iterative development: beginning with a prototype built on LangChain and advancing to a full-scale system with custom components optimised for resource efficiency and multilingual support.


\section{Prototype}
The initial prototype served as a proof-of-concept integrating core RAG elements using off-the-shelf tools \citep{mishra_using_2024,akkiraju_facts_2024}. The pipeline combined:
\begin{itemize}
      \item a CSV-based knowledge base,
      \item a FAISS vector store,
      \item the Mistral NeMo large language model,
      \item and a Streamlit-based interface.
\end{itemize}

The prototype uses LangChain for managing prompts, memory, and asynchronous streaming.\footnote{LangChain is an open-source framework designed to simplify the development of applications powered by LLMs. For further details and practical examples, consult the official documentation at \url{https://python.langchain.com/docs/introduction/}.} The interface allows users to input NL queries in Italian and receive fluent, context-aware responses.\nocite{noauthor_langchain_2024}

Evaluation was conducted via a dual approach:
\begin{enumerate}
      \item \textbf{Human Assessment}, using a 5-point Likert scale to rate and annotate consistency, fluency, completeness, and relevance \citep{abeysinghe_challenges_2024};
      \item \textbf{LLM-as-a-Judge}, where GPT-3.5 is used to auto-evaluate responses via few-shot prompting.\footnote{See also \citep{svikhnushina_approximating_2023} for method inspiration.}
\end{enumerate}

Challenges included limited scalability, resource inefficiency, lack of chunk-level metadata control and the absence of standardised
evaluation methods and benchmarks. These findings informed the redesign of the full system.

\section{Full-scale implementation}
The full system was re-engineered from scratch to support dynamic, scalable document ingestion, contextual retrieval, and high-precision answer generation using open-source LLMs. All LangChain dependencies were removed in favor of custom Python implementations to improve modularity, debugging transparency, and flexibility in processing. The final architecture includes:
\begin{itemize}
      \item a custom knowledge base construction module, which includes sitemap generation and web-crawling,
      \item semantic chunking and metadata enrichment,
      \item LLM-based vector embeddings,
      \item a FAISS vector store for retrieval,
      \item a generation module with open-source Mistral NeMo model,
      \item generative response with inline citation handling,
      \item a reactive front-end interface built on Streamlit,
      \item and a feedback management system.
\end{itemize}

\sloppy
\section{Data Acquisition and Preprocessing}
\subsection{Sitemap Generation}
The sitemap is constructed via a focused breadth-first crawler targeting the MediaWiki documentation (\url{https://gna.cultura.gov.it/wiki}) of the GNA \citep{mic_mic_2019}. 
The crawler:
\begin{itemize}
      \item starts at the root node (\texttt{\href{https://web.archive.org/web/20250803092155/https://gna.cultura.gov.it/wiki/index.php/Pagina_principale}{Pagina\_principale}}\nocite{noauthor_wiki_2025});
      \item follows internal links matching \texttt{/wiki/index.php/}, excluding namespaces such as \texttt{Special:}, \texttt{User:}, or \texttt{Talk:};
      \item removes query parameters to avoid duplicates;
      \item applies a polite crawling policy (1-second delay, custom user-agent header);
      \item imposes crawl depth (max 10) and page limits (max 200 pages);
      \item and generates a structured sitemap in XML format.
\end{itemize}

HTML is parsed using BeautifulSoup, isolating the main content via focusing on the \texttt{div} with \texttt{id="mw-content-text"}, and excluding sidebars and footers. The output is serialised into an XML file (\texttt{GNA\_\_sitemap.xml}) including last-modified timestamps, priority, and change frequency. 

This sitemap serves as the foundation for subsequent document crawling and chunking.

\subsection{Document Crawling and Chunking}
Crawling retrieves the URLs listed in the sitemap. The system fetches HTML content asynchronously, applying retries and concurrency limits. During parsing:
\begin{itemize}
      \item extraneous HTML elements are stripped;
      \item headers (\texttt{h1-h6}), paragraphs, tables, and images are preserved in semantic order;
      \item a hierarchical structure is reconstructed to retain navigational breadcrumbs.
\end{itemize}
      
Content is then chunked using a sliding window strategy of max-512 characters per chunk and 128-character overlap. Each chunk includes metadata such as \texttt{source URL}, \texttt{page title}, \texttt{section headers}, \texttt{chunk ID}, \texttt{content type} (text, table, image), \texttt{keywords}, \texttt{named entities}.\footnote{Output: 835 structured chunks saved in \texttt{data/chunks\_memory.json}.}

NER is performed using spaCy (\texttt{it\_core\_news\_md}), while keywords are extracted with KeyBERT multilingual model (\textit{paraphrase-multilingual-MiniLM-L12-v2}). Tables are chunked as standalone elements, and image references are retained for OCR via Tesseract when applicable.

This contributes to creating the knowledge base, which is stored in a JSON file (\texttt{data\/knowledge\_base.json}) for subsequent embedding, retrieval and generation tasks.

\sloppy
\subsection{Vector Embeddings}
Document chunks are converted into dense vector representations using the \textit{intfloat\/multilingual-e5-large} model from Sentence Transformers. This model was selected for its multilingual encoding capabilities and strong performance in semantic retrieval tasks, making it suitable for the predominantly Italian knowledge base while allowing also cross-lingual queries. 

Text chunks are processed in batches and transformed into L2-normalised embeddings to ensure vector magnitudes are uniform. Embeddings are cached locally to avoid redundant computation across runs. The normalised vectors are stored in a FAISS \texttt{IndexFlatIP} index, which performs brute-force nearest neighbor search using the inner product (dot product) as metric. Alongside the vector index, a separate metadata store is maintained, linking each embedding to its corresponding chunk through a unique identifier. This separation enables efficient similarity search in FAISS while preserving quick access to rich metadata such as source URL, document structure, and content type for downstream processing.\footnote{Output: FAISS index stored in \texttt{.faiss\_db}, linked with its metadata.}

These embeddings and their associated metadata form the foundation for the retrieval stage, where user queries -- also encoded with the same \textit{multilingual-e5-large} model, to guarantee that both queries and documents share the same normalised vector space -- are matched against the stored vectors to identify the most semantically relevant chunks for answer generation.

\section{Candidates Retrieval}
When a user submits a query, it is embedded using the same encoder to ensure vector space consistency. The FAISS index, configured for inner-product similarity, is queried to return the top-k candidate chunks.\footnote{The default value of \texttt{k=5} was determined empirically to balance response quality and token constraints.} Retrieval is executed entirely within the vector space to maximise speed and maintain consistent scoring across CPU-based deployments. The retrieved results are enriched with their stored metadata, which includes source URL, document title from the original web section, hierarchical section headings, and content type (text, table, image). Candidates are then grouped by provenance, ensuring that related chunks from the same source URL are passed together into the generation stage, thus improving contextual coherence, supporting inline citation, and reducing redundancy.

To further improve factual density, a lightweight filtering heuristic is applied to penalise very short or contextless chunks, deprioritising fragments that lack substantive information. The grouped and filtered candidates are returned as structured context objects, ready to be consumed by the answer generation module. 

This retrieval framework serves as the baseline for subsequent ablation studies described in the evaluation phase (cf. \autoref{sec:exp_ablation}), where alternative retrieval strategies and scoring variations are tested against this reference implementation.

\subsection{Experimental Setup for Ablation Studies}\label{sec:exp_ablation}
To systematically evaluate the contribution of different retrieval strategies. we conducted a series of ablation experiments. In this context, \textit{ablation} refers to the systematic removal, isolation, or modification of system components in order to evaluate their specific effect on overall performance. Importantly, none of the tested approaches was permanently integrated into the main pipeline; instead, each configuration was evaluated independently to allow for a broader performance comparison, as detailed in \autoref{sec:evaluation_protocol}. The results were then compared to the baseline retrieval outcomes.

The following retrieval configurations were implemented and evaluated:
\begin{itemize}
    \item \textbf{Dense retrieval.}\\
    This method employs dense vector embeddings for document representation and similarity search. It relies on FAISS as the underlying vector database, wrapped via the \texttt{VectorDatabaseWrapper} module. Queries are encoded into embeddings and compared with pre-computed document embeddings, returning the top-k results ranked by inner-product similarity. Query embeddings are cached using a normalised MD5 hash to avoid redundant computations, and batch querying is supported.
    \item \textbf{BM25 retrieval.}\\ 
    A traditional sparse retriever based on the BM25 algorithm. The index was constructed over concatenated metadata fields -- \texttt{title}, \texttt{keywords}, \texttt{headers\_context}, and \texttt{document} -- from the same chunks metadata store. Preprocessing was applied specifically for Italian, including stopword removal, stemming, and handling of clitics and apocope forms. The tokenised corpus was then indexed for lexical matching. As with dense retrieval, a batch mode was implemented. The default cutoff parameter (k = 5) was selected empirically to balance response quality against tokenisation and latency constraints.
    \item \textbf{Hybridetrieval.}\\ 
    To combine semantic similarity with lexical matching, hybrid retrieval strategies were explored. Two fusion techniques were implemented:
    \begin{itemize}
            \item \textbf{Weighted Reciprocal Rank Fusion (RRF):} ranks from dense and sparse retrievers are aggregated using RRF. The fusion score for document $d$ is computed as:
\[
\frac{w_\mathrm{dense}}{k + \mathrm{rank}_\mathrm{dense}} + \frac{w_\mathrm{sparse}}{k + \mathrm{rank}_\mathrm{sparse}}
\]
where the default weights parameters were $w_{\text{dense}} = w_{\text{sparse}} = 1.0$, candidates set size = 50, and \text{\texttt{k}} = 60.
            \item \textbf{Score-blend fusion:} here, normalised scores from the two retrievers are merged using a custom blending function that allows for fine-tuning the influence of each method:
\[
S_{\text{norm},d} = \frac{S_d - \min(S_d)}{\max(S_d) - \min(S_d)}, \quad  
S_{\text{norm},s} = \frac{S_s - \min(S_s)}{\max(S_s) - \min(S_s)}
\]
\[
S_h = S_{\text{norm},d} + \alpha \cdot S_{\text{norm},s}
\]
where $S_{\text{norm},d}$ and $S_{\text{norm},s}$ are the min–max normalised scores of the dense and sparse retrievers, respectively, and $\alpha$ controls the relative weight of the sparse component ($w_d = w_s = 1$, $k = 60$) \citep{wang_searching_2024}. Unlike RRF, this approach requires that scores from different retrievers are defined on comparable scales.
\end{itemize}
\end{itemize}
%\noindent\subsection*{\textit{\Large What to pick?}}
\vspace{1em}
\noindent{\textit{\Large What to pick?}}

The choice between RRF and Score-blend depends on the specific retrieval context. RRF is particularly suitable when score scales between retrievers are incompatible or unstable, as its rank-based aggregation is less sensitive to scale differences and prioritises consensus across retrieval methods. Conversely, Score-blend is more appropriate when per-query scores are reliable, as it allows finer control over the relative influence of dense and sparse components. Edge-case behaviors include: 
\begin{enumerate}[(a)] 
\item \textit{Document appears in only one list}: RRF ranks it lower, while Score-blend assigns it a normalised score from the contributing retriever;
\item \textit{All scores equal in a list}: Score-blend reduces the contribution of that retriever, whereas RRF still differentiates documents by rank.
\end{enumerate}


\noindent \subsubsection*{\Large Query Rewrite}
To sharpen retrieval quality, we experimented with \textit{query rewriting}, i.e. reformulating user queries to increase the likelihood of retrieving relevant documents. Within these experiments, query rewriting was conceived as a multi-strategy process that generates alternative query variants through complementary transformations, each targeting different aspects of query understanding and manipulation \citep{li_dmqr-rag_2024}. Specifically, the approach integrates:
\begin{itemize}
      \item \textbf{Core Content Extraction (CCE)} -- a sequence-to-sequence transformation using the \textit{it5-small} model, that rewrites the query to capture its essential informational content while removing peripheral terms.
      \item \textbf{Keyword Expansion (QE)} -- extraction of key terms with KeyBERT, followed by enrichment through n-gram combinations and synonym substitutions to introduce semantically related expressions.
      \item \textbf{General Query Rewriting (GQR)} -- linguistic normalisation based on spaCy lemmatisation and stopword removal, yielding canonical query forms.
      \item \textbf{Pseudo-Relevance Feedback (PRF)} -- top-ranked documents from an initial retrieval pass are analysed to extract additional high-frequency terms not present in the original query, which are then appended to the original query to form an expanded version.
      \item \textbf{Query Decomposition} -- conjunctive or disjunctive queries are split into simpler sub-queries, each addressing a distinct semantic aspect.
\end{itemize}

These strategies can be applied individually or in combination (\texttt{strategy="all"}), producing a set of reformulated queries submitted to the base retriever (Dense, BM25, or Hybrid).\\


\noindent\subsubsection*{\Large Reranking}

Finally, a \textit{reranking} stage was introduced to refine retrieval outcomes. Reranking is a post-retrieval process that reorders an initial set of candidate documents based on a more precise estimation of their relevance. In our implementation, this component operates as a wrapper around a base retriever (Dense, BM25, or Hybrid) and uses a transformer-based cross-encoder model (\textit{cross-encoder/ms-marco-MiniLM-L-6-v2}) to jointly encode the query and each candidate document, assigning a contextual relevance score. Unlike the base retriever, which typically evaluates query–document similarity using independent embeddings or lexical term matching, the cross-encoder considers full cross-attention between query and document tokens, thereby capturing finer semantic relationships.

At runtime, the reranker receives the top-N candidates (with \texttt{N} = 50) from the base retriever, tokenises each query–document pair, and performs inference in batches with mixed-precision support when available. The final output consists of a top-k ranked list reordered by cross-encoder scores. This stage provided a more precise estimation of relevance, compensating for weaknesses in both dense and sparse retrieval alone.


\section{Generation}
The generation phase employs Mistral NeMo,\footnote{\url{https://web.archive.org/web/20250803120348/https://mistral.ai/news/mistral-nemo}.\nocite{noauthor_mistral_2025}} an open-source LLM accessible via a dedicated Mistral API and hosted independently. The choice of this model was driven by multiple factors:
\begin{itemize}
      \item open-source availability and permissive license,
      \item strong performance on multilingual tasks,
      \item low latency and high throughput on CPU hardware,
      \item availability of an official API for direct deployment integration.
      \item ability to handle long context windows -- e.g., up to 128 thousands tokens, which is sufficient for processing multiple retrieved chunks.
\end{itemize}

Among available open-source LLMs evaluated -- e.g., LLaMA 3, OpenAI, Falcon --, only Mistral NeMo satisfied all criteria in terms of language coverage, response control, and reproducibility, while also providing infrastructure for embeddings fine-tuning and evaluation in research contexts.\footnote{For an overview between open-source and proprietary models, read \textcite{noauthor_open_2025}.}

The generation module is designed to produce fluent, context-aware answers with inline citations. It uses a custom prompt template that includes system instructions (see \autoref{sec:prompt_engineering}), user query, top-k retrieved chunks (grouped and cited), chat history and memory (to support context-aware follow-up).

The API request includes temperature, top-p, and max token constraints, with defaults of:
\begin{itemize}
      \item temperature = 0.3, to ensure factuality;
      \item top-p = 0.9, to control diversity;
      \item max-tokens = 512, to limit response length.
\end{itemize}

Responses are post-processed to verify the inclusion of inline citations, language alignment for Italian, and basic formatting (e.g., numbered citations, paragraph boundaries).

\subsection{Prompt Engineering Techniques} \label{sec:prompt_engineering}
The system uses structured prompt engineering to ensure accurate, traceable, and contextually coherent answers. The prompt template is dynamically generated with the following components:

\noindent\subsubsection*{\large System Instructions, Boundaries and Constraints}
A custom system message (\autoref{lst:system_prompt}) is injected at the top of the prompt to guide the model’s behavior. This message instructs the system to enforce neutrality in its answers, prioritise relevant and verifiable information, and include inline citations in square brackets that correspond to metadata entries. It also explicitly discourages hallucinations and speculative responses.

\vspace{0.6em}
\begin{lstlisting}[breaklines=true,
                  frame=none,
                   caption={System prompt specifying assistant constraints and response instructions.},
                   captionpos=b,
                   label={lst:system_prompt}]
system_content = """
    Sei un assistente virtuale incaricato di rispondere a domande sul manuale operativo del Geoportale Nazionale per l'Archeologia (GNA), gestito dall'Istituto Centrale per il Catalogo e la Documentazione (ICCD).

    Segui sempre queste regole:
    1. Non rispondere a una domanda con un'altra domanda.
    2. Rispondi **sempre** in italiano, indipendentemente dalla lingua della domanda, a meno che l'utente non richieda esplicitamente un'altra lingua.
    3. Cita le fonti utilizzando la notazione [numero] dove:
        - Le fonti sono fornite nel contesto della domanda e sono numerate in ordine crescente;
        - Usa numeri diversi per fonti diverse;
        - Non includere mai l'URL nel corpo della risposta;
    4. Alla fine della risposta, aggiungi un elenco di riferimenti con il seguente formato, su righe separate:
        [ID] URL completo
    5. Se non hai informazioni sufficienti per rispondere, rispondi "Non ho informazioni sufficienti".

    Le tue risposte devono essere sempre:
    - Disponibili, professionali e naturali
    - Grammaticalmente corrette e coerenti
    - Espresse con frasi semplici, evitando formulazioni complesse o frammentate
    - Complete e chiare, evitando di lasciare domande senza risposta
"""
\end{lstlisting}
\vspace{0.5em}


\noindent\subsubsection*{\large Dynamic Citation Handling}

Each chunk passed to the LLM is numbered and grouped with its metadata (title, URL). When generating a response, Mistral is instructed to cite only the chunks used, ensuring traceability. Post-processing checks for unmatched citations or unreferenced metadata.

This modular prompting strategy proves crucial for maintaining factual consistency while supporting multilingual input and long-form reasoning.


\section{Evaluation Protocol}\label{sec:evaluation_protocol}
Evaluation was conducted across two complementary dimensions: 
\begin{itemize}
      \item \textbf{Quantitative}, focusing on retrieval performance through metrics such as Recall (R@), Mean Reciprocal Rank (MRR@), Normalised Discounted Cumulative Gain (nDCG@), Average Precision (AP@), and Latency to assess retrieval performance;
      \item \textbf{Qualitative}, assessing the perceived quality and usability of responses through human feedback.
\end{itemize}

This dual perspective reflects the understanding that effective RAG-based QASs require not only accurate retrieval and generation but also operational efficiency and adaptability in real-world contexts \citep{akkiraju_facts_2024}. To support continuous refinement, the evaluation was designed as an iterative process, with quantitative results informing system optimisation and qualitative insights guiding user-centred adjustments.

All retrieval configurations (Dense, BM25, Hybrid, Query Rewriting, and Reranking variants) were evaluated under the same protocol, ensuring comparability of results across ablation studies.

\subsection{Structural Limitations}

The evaluation faced a number of structural constraints:
\begin{itemize}
      \item \textbf{Absence of a gold standard:} there was no authoritative or verified set of annotated responses to serve as a benchmark of correctness.
      \item \textbf{No baseline system:} internal institutional tasks had no legacy solutions or established benchmarks for direct comparison.
      \item \textbf{Limited availability of domain experts:} during the early phases, development was conducted without input from real users or expert annotators; human feedback was integrated only at a later stage.
      \item \textbf{Limited applicability of automated generation metrics:} common algorithmic measures such as BLEU, ROUGE, and METEOR, although widely applied in text generation, have been shown to be ineffective for dialogue and QA evaluation \citep{deriu_survey_2020,liu_how_2016}.
\end{itemize}

These constraints mirror challenges identified in recent RAG evaluation literature, which emphasises the lack of standardised protocols and the importance of balancing intrinsic metrics with human-centred evaluation (Guo et al., 2023; Mehri and Eskenazi, 2020; Abeysinghe, 2024). Akkiraju et al. (2024) further highlight the need to assess both accuracy and efficiency, pointing to critical control points in the RAG pipeline where trade-offs between retrieval quality and latency emerge.


\subsection{Datasets}\label{sec:datasets}

Two synthetic evaluation sets were created to support systematic analysis:
\begin{itemize}
      \item \textbf{Single-hop dataset:} containing 508 queries designed to elicit single-document answers, each with a single gold document (cf. \autoref{lst:single-hop-example}). This dataset tested the system's ability to retrieve and generate answers based on isolated chunks of information.
      \item \textbf{Combined dataset:} containing 400 additional queries that require multi-hop reasoning, where answers are derived from multiple documents (2-4 chunks), for a total of 908 queries entries (cf. \autoref{lst:combined-set-example}). This dataset assessed the system's capacity to integrate information from various sources and generate coherent contextual responses.
\end{itemize}


\begin{lstlisting}[frame=none,
                   caption={JSON output format for single-hop dataset items.},
                   captionpos=b,
                   label={lst:single-hop-example},
  xleftmargin=0.2\textwidth,
  xrightmargin=0.2\textwidth]
{
  "question": "<Italian question>",
  "relevant_docs": ["<chunk_id>"],
  "document_content": "<chunk text>"
}
\end{lstlisting}

\vspace{0.6em}

\begin{lstlisting}[frame=none,
                   caption={JSON output format for combined dataset items, including single-hop and multi-hop questions.},
                   captionpos=b,
                   label={lst:combined-set-example},
  xleftmargin=0.05\textwidth,
  xrightmargin=0.05\textwidth]
{
  "question": "<Italian question>",
  "relevant_docs": ["<chunk_id_1>", "<chunk_id_2>", "..."],
  "document_content": ["<text_1>", "<text_2>", "..."],
  "is_multihop": false|true,
  "num_docs": 1|2|3|4
}
\end{lstlisting}
\vspace{0.6em}

Together, the datasets provided balanced coverage of both simple and complex retrieval scenarios. In both cases, questions were LLM-generated directly from the chunks corpus, with explicit prompt constraints to ensure grounding in the available content.


\subsection{Metrics}\label{sec:metrics}


\begin{itemize}
    \item R@5 (Recall at 5): Measures the fraction of relevant documents retrieved in the top 5 results.
    \item MRR (Mean Reciprocal Rank): Evaluates the rank position of the first relevant document.
    \item nDCG@5 (Normalised Discounted Cumulative Gain at 5): Measures ranking quality, emphasising higher placement of relevant results.
    \item AP@5 (Average Precision at 5): Computes the average of precision values at each relevant document within the top 5.
    \item Latency: Average retrieval time per query (in seconds).
\end{itemize}

\subsubsection*{Reproducibility Metadata}

Each evaluation run stores a machine-readable report (JSON) capturing: dataset name (single-hop / multi-hop), creation timestamp (UTC), orchestrator and model identifiers (Mistral API model name), retrieval top-k and candidate-k settings, batch size, and device configuration. These artefacts enable precise traceability of conditions across ablation configurations (Dense, BM25, Hybrid, Query Rewriting variants, and Reranking).

\subsection{Qualitative Assessment}
To complement intrinsic metrics, qualitative evaluation focused on dimensions more directly linked to user experience:
\begin{itemize}
      \item \textbf{Relevance:} Whether the generated answer addressed the query meaningfully.
      \item \textbf{Fluency:} Linguistic naturalness and readability of responses.
      \item \textbf{Completeness:} Coverage of the key information needed to satisfy the query.
      \item \textbf{Usability:} Perceived usefulness of the system as an interactive tool.
\end{itemize}

Human feedback was collected using a lightweight rating interface (3-point Likert-scale scoring), later exported as structured datasets for analysis. Although limited in scale, this qualitative perspective provided insights into aspects of response quality that purely algorithmic metrics could not capture, particularly in relation to user trust and system transparency.



\section{User Interface}
The user interface (UI) is implemented using Streamlit, selected for its fast prototyping capabilities, built-in support for asynchronous processing, and integration with Python-based NLP components. The front end acts as the primary touchpoint between users and the GNA QA system, displaying generated answers with inline citations, and collecting user feedback.

Why the UI matters methodologically (collecting user feedback, enabling qualitative eval).

One screenshot max;

\subsection{Streamlit User Experience}
The Streamlit app is organised into three main areas:
\begin{enumerate}
      \item \textbf{Sidebar:} Contains MiC refernce, institutional links to the GNA documentation, and contextual help describing the assistant’s capabilities. It also provides functional controls including a \texttt{Clear Chat History} button to reset the session (\texttt{st.session\_state.chat\_history}) and a \texttt{Download Feedback} button for exporting user queries and system's responses.
      \item \textbf{Main interface:} Provides a natural language input field (\texttt{st.chat\_input}) for querying the assistant. It displays the chat history, including:
      \begin{itemize}
            \item user messages,
            \item assistant responses (formatted via \texttt{st.chat\_message}),
            \item feedback buttons (3 points Likert-scale) for each assistant reply.
      \end{itemize}
      \item \textbf{Session features}: 
      \begin{itemize}
            \item Chat history is limited to the most recent 10 exchanges (\texttt{MAX\_HISTORY}), which are stored and updated in \texttt{st.session\_state};
            \item Feedback from individual message indexes is stored as a set (\texttt{st.session\_state.feedback\_given}), allowing the system to prevent duplicate ratings and dynamically update UI feedback (e.g., ``Valutazione registrata con successo.'').
      \end{itemize}
\end{enumerate}

\subsection{Session State Management}
To ensure smooth, stateful interactions and to minimise computational overhead, the system makes extensive use of \texttt{st.session\_state}. This approach allows chat memory for both user and assistant messages to persist, enables caching of API results -- including responses generated by Mistral and citation mappings -- and facilitates the tracking of feedback.\footnote{For more details on Streamlit session state management, see \url{https://docs.streamlit.io/}.\nocite{noauthor_streamlit_2025}} In addition, the system incorporates Python’s \texttt{asyncio} and \texttt{concurrent.futures} modules to support non-blocking retrieval and generation. For each interaction with the language model, a new event loop is created to maintain compatibility with Streamlit’s execution model, and the query to the model is run in a thread-safe environment using a \texttt{ThreadPoolExecutor}. This pattern guarantees UI responsiveness and provides a fluid user experience even under high network latency or API response times.

\section{Feedback Loop}
To support iterative improvement of the assistant and promote user engagement, the system integrates an interactive feedback module that allows users to rate each answer directly within the Streamlit interface. This design is intended to support continuous quality assessment and transparent evaluation of LLM-generated content.

\subsection{Collection}
Each assistant response is immediately followed by three clickable UI buttons in the form of a 3-point Likert scale, enabling users to provide a quick evaluation:
\begin{itemize}
\item 1 star $\star$ - Poor: The answer is incorrect, incomplete, or irrelevant.
\item 2 stars $\star\star$ - Fair: The answer is partially correct but lacks clarity or depth.
\item 3 stars $\star\star\star$ - Good: The answer is accurate, complete, and well-structured.
\end{itemize}

This mechanism is implemented using Streamlit’s interactive widgets. Once a rating is submitted, the system prevents duplicate feedback using an in-memory tracking set (\texttt{st.session\_state.feedback\_given}). The interface then displays a confirmation message (e.g., ``\textit{Valutazione registrata con successo.}''), improving transparency.

\subsection{Storage and Export}
Feedback is stored locally in a SQLite database (\texttt{feedback.db}) with the following schema:

\begin{figure}[H]
\begin{Verbatim}[breaklines=true]
                  CREATE TABLE feedback (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        message_index INTEGER NOT NULL,
                        question TEXT NOT NULL,
                        answer TEXT NOT NULL,
                        rating INTEGER NOT NULL
                  );
\end{Verbatim}
\caption{Schema of the feedback's SQL databse.}\label{fig:sql_schema}
\end{figure}

This structure supports reproducibility and traceability by maintaining a clear mapping between: the user's query string, the generated response, the rating score (1-3) and the associated timestamp (i.e., time of submission). All records are saved with minimal overhead, using parameterised SQL insertions and transaction-safe commits.\\

To ensure long-term preservation and collaborative accessibility of user feedback, the system implements a mechanism for periodic synchronization of collected feedback with a persistent repository. This setup enables version control over user interaction logs, supports iterative evaluation by external reviewers, and enables rollback and comparison across model updates.\footnote{This implementation is intended for controlled research use only. For production environments, secure alternatives such as authenticated APIs and hardened database infrastructures are recommended to ensure data safety and compliance with privacy standards.}\\

From the sidebar, users can export all feedback as a \texttt{.csv} file using the \textit{``Esporta feedback''} button. This functionality is powered by the \texttt{export\_feedbacks()} function, which queries the database and converts it to a downloadable format using Pandas. 

This export can be used by researchers, developers, or project coordinators to assess the assistant's performance over time.


\section{Resource and Deployment Constraints}
Deploying a multilingual RAG workflow with different language models integrated and semantic search poses non-trivial challenges for environments lacking access to GPUs or large memory allocations. To ensure the GNA QA system remains responsive and cost-efficient, especially for open usage, several optimization strategies were integrated into the pipeline.

\subsection{Memory Management}
To reduce RAM usage during embedding, retrieval, and generation, the following practices were adopted:
\begin{itemize}
\item \textbf{Garbage collection routines:} Explicit calls to Python's garbage collector (\texttt{gc.collect()}) were introduced to free unused memory between embedding and response generation steps.
\item \textbf{Batch processing:} Document chunks are processed in batches to minimise memory overhead, especially during retrieval.
\item \textbf{Lazy loading:} All embeddings are computed once and stored to disk. On app startup, only metadata is loaded, and the FAISS index is memory-mapped to reduce RAM footprint.
\item \textbf{Asynchronous processing:} Streamlit's async capabilities allow the UI to remain responsive while background tasks are executed, preventing memory spikes during long-running operations.
\item \textbf{Cache clearing policies:} \texttt{st.session\_state} objects are pruned after each session or on manual reset by the user to prevent memory bloating during prolonged use.
\end{itemize}

\subsection{Computational Constraints Mitigation}
Given the constraint of CPU-only deployments on platforms like Streamlit Cloud, the following strategies were implemented:
\begin{itemize}
\item \textbf{Model selection:} The use of \textit{intfloat/multilingual-e5-large} for embedding provides a trade-off between semantic accuracy and compute efficiency, even without GPU acceleration.
\item \textbf{API offloading:} Offloading generative tasks to an external API prevents the local system from being overloaded and allows scaling independently of frontend performance.
\item \textbf{Timeout and fallback handlers:} If the generation request exceeds 10 seconds or fails -- e.g., due to API rate limits --, the app returns a graceful fallback response, allowing users to retry or simplify their query without crashing the app.
\item \textbf{Asynchronous I/O:} For embedding, retrieval, and response generation, asynchronous requests reduce UI freezing and ensure smoother user experience even under high latency conditions.
\end{itemize}


\section{Ethics and Data Governance}
The GNA QA system is designed with a strong emphasis on ethical considerations and data governance, particularly in the context of cultural heritage and public information. Key principles include:
\begin{itemize}
\item \textbf{Transparency:} The system provides clear information about its data sources, methodologies, and limitations, ensuring users understand how answers are generated and the provenance of information.
\item \textbf{Privacy:} The system avoids processing personally identifiable information (PII) and ensures that all data used is publicly available or explicitly licensed for use in research and educational contexts.
\item \textbf{Licensing:} All components, including the knowledge base, models, and software libraries, are selected based on permissive licenses that allow for academic and non-commercial use, ensuring compliance with legal and ethical standards.
\item \textbf{Auditability:} The system maintains logs of user interactions, feedback, and system performance, enabling ongoing evaluation and improvement while respecting user privacy and data protection regulations.
\end{itemize}
These principles guide the development and deployment of the GNA QA system, ensuring it serves as a responsible and ethical tool for cultural heritage public engagement.
Provenance, PII avoidance, licensing, auditability.


\end{spacing}
