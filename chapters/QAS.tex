\chapter{Overview~of~Question-Answering~Discipline}
\label{chap:QAS}
\sloppy
\begin{spacing}{1.5}
\setcounter{table}{0}
This chapter introduces the foundations of question answering (QA) as both a computer science discipline and an applied technology. Before the emergence of large language models (LLMs)\footnote{Large Language Models (LLMs) are advanced AI systems trained on massive text datasets to generate and understand human language. For an accessible overview, see \href{https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e}{\textit{A Very Gentle Introduction to Large Language Models without the Hype}} \citep{riedl_very_2023}.}, Transformers\footnote{The Transformer is a neural network architecture introduced in 2017 that efficiently models sequential data using a self-attention mechanism. The original paper, \textit{Attention Is All You Need} by Vaswani et al. (\citeyear{vaswani_attention_2017}), provides a foundational introduction.}, and modern generative AI\footnote{Generative AI refers to systems capable of producing new content, such as text, images, or audio, based on learned patterns. For more, see the \textit{Stanford AI Index 2025 Report} \citep{maslej_artificial_2025}.}, question-answering systems (QAS) progressed through distinct paradigms : from symbolic and rule-based architectures to traditional information retrieval (IR) models and early neural networks approaches \citep{jurafsky_chapter_2024,antoniou_survey_2022}. Early systems depended on domain-specific models, manually curated knowledge bases, keyword retrieval, or engineered features. In recent years, pre-Transformer language models such as BERT and GPT-2/3 have answered questions by extracting or generating responses from static training data alone \citep{caballero_brief_2021}. While efficient and easy to deploy, these models exhibit factual inaccuracies, shallow contextual understanding, and limited adaptability to new or evolving information \citep{alanazi_question_2021}. They also frequently hallucinate or generate outdated responses, constrained by their static training corpora.


\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash\bfseries}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
\toprule
\textbf{Models} & \textbf{QA Approach} & \textbf{Examples / Results}\\
\midrule
Symbolic / Rule-based (1960s–1980s) & Rule-based, domain-specific, handcrafted knowledge base & BASEBALL, LUNAR, SHRDLU \\
Early IR Approaches (1990s–mid-2010s) & Keyword retrieval, TF-IDF, BM25, open-domain ranking & TREC QA \\
Statistical / Seq2Seq (2000s–2018) & N-gram, embeddings, RNN/LSTM, statistical IR & Early neural QA, Reading comprehension in 2010s \\
Transformer-based & Pre-training, fine-tuning, self-attention & BERT (93\% F1 on SQuAD), XLNet \\
Generative LLMs and agents & Prompting, retrieval-augmented generation, agentic reasoning & GPT-3, RAG pipelines \\
\bottomrule
\end{tabularx}
\caption{Evolution of QA systems.}
\label{tab:qa_evolution}
\end{table}

\section{Pre-Transformer Era: Symbolic and Statistical Systems}
The development of QAS prior to the rise of Transformers was shaped by several key methodological shifts and technological milestones. The earliest efforts prioritized manually curated knowledge bases and rules-based systems for precise but limited question matching. As the scope of QA expanded, techniques evolved to incorporate large-scale information retrieval methods, statistical modeling, and increasingly complex approaches to feature engineering and answer extraction. This trajectory ultimately set the stage for early neural models that leveraged word embeddings and sequence modeling, gradually moving the discipline toward data-driven architectures and deeper semantic representation. The following sections trace these major trends, illustrating how each contributed to the capabilities and limitations of pre-Transformer QA systems.

\subsection{Rule-Based Systems (1960s--1980s)}
Early QAS relied on highly constrained, domain-specific approaches built around manually constructed knowledge bases. These systems operated within carefully delineated boundaries, matching user questions to a limited set of predefined templates and answer patterns. While this design enabled highly precise responses in their target domains, it also rendered the systems brittle and inflexible -- minor variations in user queries or topics outside the encoded scope often resulted in failure to provide meaningful answers.

Expert systems from this era encoded explicit inference rules and logical representations of knowledge, enabling a form of automated reasoning that was fundamentally deterministic. However, these approaches struggled to address ambiguity or generalize beyond the hand-curated domain, and could not scale to larger, more dynamic information environments \citep{noauthor_question_2025, jurafsky_chapter_2024}.

Seminal examples of early domain-specific QA systems include:
\begin{itemize}
    \item \textbf{BASEBALL} (1960s): Hand-coded rules and database logic for Major League Baseball\footnote{Major League Baseball (MLB) is the leading professional baseball league in North America. It is regarded as the world’s premier baseball competition.} questions \citep{green_baseball_1961}.
    \item \textbf{LUNAR} (1971): Pattern matching and restricted knowledge base for geological questions about Moon rocks \citep{woods_lunar_1972}.
    \item \textbf{SHRDLU}\footnote{SHRDLU was developed at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) between 1968--70. The software allowed users to interact conversationally with a program that could manipulate, describe, and answer questions about objects in a virtual \"blocks world\", a simplified environment containing various movable blocks. Read more about SHRDLU program here: \url{https://hci.stanford.edu/winograd/shrdlu/}.} (late 1960s): Symbolic reasoning for a blocks-world robot in a toy domain \textcolor{blue}{(Winograd, 1971)}.
    \item \textbf{Unix Consultant (UC)}\footnote{UC (QA) system, created at U.C. Berkeley (CA), answered queries about the Unix operating system using a hand-crafted knowledge base and could tailor responses to different user types \citep{robert_berkeley_1988}.}  and \textbf{LILOG}\footnote{LILOG project was as a text-understanding system designed for tourism information in a German city \citep{noauthor_question_2025}.} (1980s): Domain-specific QA via linguistic rules and expert knowledge; though both projects remained at the demonstration stage, they contributed to advanced research in computational linguistics.
\end{itemize}

These early QA systems demonstrated the potential of automated question answering but highlighted the central challenge of balancing precision with generality and scalability. Their evolution would motivate the subsequent shift toward statistical and data-driven approaches \citep{jurafsky_chapter_2024, antoniou_survey_2022}.

\subsection{Traditional Information Retrieval Approaches (1990s--mid-2010s)}
As the volume of unstructured web data grew, QA moved toward ranking text passages with IR techniques like TF-IDF\footnote{TF-IDF (Term Frequency–Inverse Document Frequency) is a statistical method for ranking how important a word is to a document in a collection.} and BM25,\footnote{BM25 is a ranking function that improves information retrieval by considering term frequency, document length, and saturation effects.\\For more details on TF-IDF and BM25, read \textit{Introduction to Information Retrieval} \citep{manning_introduction_2008}.} to locate relevant content within large text collections. Open-domain QA systems -- such as those in TREC QA\footnote{TREC QA refers to the Question Answering track of the Text REtrieval Conference (TREC), a long-running evaluation series that has set benchmarks for open-domain QA research since 1999. See \url{https://trec.nist.gov/data/qa.html}\notecite{noauthor_text_nodate}} \citep{hirschman_natural_2001} -- shifted the focus from structured fact retrieval to returning ranked sentences or extracting answer spans from retrieved passages. These approaches made it possible to scale QA to a broad range of topics and data sources, yet they also introduced notable challenges. Lacking deep understanding of natural language, IR-based QA systems often failed to interpret nuances, synonyms, or complex phrasing, and frequently missed correct answers that did not explicitly match the user’s query terms \citep{antoniou_survey_2022, caballero_brief_2021}.

\subsection{Statistical Models and Feature Engineering (2000s--2018)}
During the 2000s and 2010s, the adoption of n-gram models and statistical IR approaches (cf. TF-IDF, BM25, probabilistic models\footnote{Language Models for IR (LMIR) -- such as n-gram models -- estimate the probability of a query being generated by a document's language model. They capture local word dependencies and were widely used in early QA, speech recognition, and spelling correction, \citep{ponte_language_1998} but were later outperformed by models like RNNs, LSTMs, and Transformers due to their limited handling of long-range context.}) enabled reasoning over large corpora, moving beyond hand-crafted rules and enabling automated extraction of candidate answers from vast, unstructured datasets \citep{manning_introduction_2008}. The introduction of word embeddings (Word2Vec, GloVe) marked a significant advancement by capturing semantic similarities between words, thereby allowing models to generalize beyond simple keyword matching. These dense vector representations supported the emergence of recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), which facilitated more accurate modeling of sequence and context in reading comprehension and retrieval-based QA tasks \citep{jurafsky_chapter_2024}. 

A major milestone in this era was IBM's \textit{Watson} system, which achieved notable success by winning the \textit{Jeopardy!} challenge in 2011.\footnote{The \textit{Jeopardy! challenge} was a high-profile test where IBM \textit{Watson} competed on the American television quiz show \textit{Jeopardy!} against two of the show's greatest human champions. Watson’s victory demonstrated significant progress in machine comprehension and open-domain question answering (\href{https://en.wikipedia.org/w/index.php?title=IBM_Watson&oldid=1301611671}{Wikipedia IBM Watson}). In February 2013, IBM announced that \textit{Watson}'s first commercial deployment would assist with utilization management decisions for lung cancer treatment at Memorial Sloan Kettering Cancer Center in New York City, in partnership with WellPoint (now Elevance Health) \citep{upbin_ibms_2013}.} Watson’s \textit{DeepQA} architecture integrated hundreds of NLP, IR and ranking components, employing sophisticated pipelines to analyze and combine evidence from diverse sources \citep{ferrucci_building_2011}. However, despite its advanced design, \textit{Watson} relied on non-generative methods; it synthesized and ranked candidate answers but did not generate free-form responses from scratch.

Simultaneously, semantic QA systems began to emerge, mapping natural language questions to structured queries (e.g. using SPARQL language) executed over knowledge bases like Freebase and DBpedia. These systems required advanced components for entity recognition, relation extraction, and reasoning over symbolic representations. Typical architectures included steps like question analysis, sentence mapping, disambiguation, and query building, enabling automatic translation of natural language into formal queries over RDF data sources. Thanks to the usage of ontology-mapping and linguistic resources (e.g., WordNet), these approaches further bridged the gap between unstructured text and structured knowledge bases \citep{franco_ontology-based_2020}.

Throughout this period, feature engineering played a central role. Techniques such as conditional random fields (CRFs) and support vector machines (SVMs) enabled models to exploit hand-crafted features -- including lexical overlap, question type, and answer patterns—to enhance answer extraction from retrieved texts. Hybrid QA systems appeared, combining keywords-based information retrieval methods for unstructured sources with knowledge-base querying for fact-based answers, thereby improving both coverage and precision \citep{antoniou_survey_2022}.

This period laid essential groundwork for the deep learning and neural approaches that would soon transform the QA landscape, highlighting the importance of both statistical modeling and intelligent feature design.

\subsection{Early Neural and Generative Models (Late 2010s)}
The late 2010s witnessed the adoption of neural architectures in question answering, building upon the foundational use of word embeddings and recurrent neural networks (RNNs). Embedding methods such as Word2Vec and GloVe allowed systems to capture deeper semantic relationships between words, providing a richer representation of both questions and candidate answers. RNNs, and their improved variants like long short-term memory networks (LSTMs) and gated recurrent units (GRUs), facilitated sequential modeling of language, enabling systems to better process and compare question and answer pairs based on their context within a sentence or passage.

Despite these advancements, early neural QA models still faced significant limitations. The reliance on RNNs restricted their ability to effectively model long-range dependencies in text, often resulting in incomplete understanding when questions required reasoning across multiple sentences or broader contexts. While neural models improved matching between questions and answers, their performance remained constrained by the size and variety of the training data.

Around this time, encoder-decoder architectures began to appear in QA research, drawing inspiration from their success in machine translation. These generative models aimed to produce answers by generating sequences of text, rather than simply extracting passages from a source document. However, early generative QA systems often struggled with factual consistency: they had a tendency to copy or paraphrase the input text rather than synthesizing novel, precise answers. Additionally, these models sometimes hallucinated information or failed to maintain logical coherence in their generated responses, limiting their reliability in open-domain settings \citep{caballero_brief_2021}.

These developments set the stage for the subsequent breakthroughs brought about by attention mechanisms and transformer-based architectures, which dramatically improved the handling of context and factuality in generative QA.

\section{Limitations of Earlier Question-Answering Methods}
Earlier approaches to question answering were hindered by several fundamental limitations. Most notably, symbolic and rule-based systems suffered from severe domain restrictions, as their performance relied on hand-crafted knowledge bases and rigid rules that did not generalize well to new or broader topics \citep{alqifari_question_2019}. The brittleness of these systems was further exposed by their heavy dependence on template matching, which frequently led to failures when users phrased questions in unanticipated ways or employed linguistic variations \citep{hirschman_natural_2001}. Information retrieval (IR) and statistical models, while more scalable, continued to struggle with true semantic understanding and contextual reasoning, often retrieving only superficially relevant snippets rather than synthesizing comprehensive or contextually rich answers \citep{alanazi_question_2021, diefenbach_core_2018}. The answers these systems produced were typically shallow, extracted verbatim from source texts rather than generated or adapted to the user’s specific information need \citep{hirschman_natural_2001,alqifari_question_2019}.

Substantial manual effort was required to design, maintain, and update rules, features, and parsers, creating significant bottlenecks and making adaptation to new domains costly and time-consuming \citep{alanazi_question_2021}. In addition, IR and knowledge base approaches frequently exhibited incomplete coverage, missing relevant answers due to differences in phrasing or limitations in their underlying datasets \citep{diefenbach_core_2018}. Early neural models, despite improvements, were generally confined to handling short text spans and struggled with complex or multi-sentence reasoning tasks. Finally, all these methods exhibited a strong dependence on the quantity and quality of available training data and engineered features, resulting in brittle and inconsistent performance across different domains and question types \citep{alanazi_question_2021, alqifari_question_2019, diefenbach_core_2018, hirschman_natural_2001}

These cumulative factors left pre-generative QA systems largely inflexible and brittle, with limited ability to provide context-aware, nuanced, or creative responses to user queries.

\section{Transformer Models and Pre-Generative AI Advances}
The advent of the Transformer architecture fundamentally transformed the landscape of neural question answering systems. Introduced by Vaswani et al. (2017), Transformers replaced recurrent neural networks with a self-attention mechanism that could model relationships between words regardless of their distance in the input sequence. This innovation allowed for efficient parallelization during training and inference, dramatically improving the scalability and performance of language models on a range of natural language processing tasks, including QA.

One of the earliest and most influential Transformer-based models was BERT (Bidirectional Encoder Representations from Transformers), introduced in 2018. BERT utilized a bidirectional attention mechanism and was pre-trained using a masked language modeling objective, allowing it to capture complex context from both directions in a sentence. When fine-tuned for question answering benchmarks such as SQuAD, BERT achieved unprecedented accuracy, surpassing previous neural models and establishing a new standard for extractive QA \citep{devlin_bert_2019}.

Building on this foundation, subsequent models explored variations and enhancements of the Transformer paradigm. XLNet, for example, employed a permutation-based language modeling objective, enabling it to better capture bidirectional context and achieve state-of-the-art results on several QA benchmarks \citep{yang_xlnet_2020}. In specialized domains, models such as BioBERT extended the BERT architecture with additional pre-training on biomedical texts, achieving top performance on domain-specific challenges like the BioASQ question answering competition \citep{yoon_pre-trained_2019}.

Parallel research into model architectures also produced frameworks such as Dynamic Coattention Networks (DCN), which fused question and context representations through attention mechanisms and iterative decoding, further improving accuracy on reading comprehension tasks \citep{xiong_dynamic_2018}.

The rise of Transformers prompted a fundamental methodological shift across the QA field. Surveys in literature underscore a pronounced move toward both extractive and generative QA pipelines, with each stage -- chunking, embedding, retrieval, and answer generation -- now explicitly modeled and systematically optimized. At the same time, interest in conversational and multi-turn QA has grown rapidly, as Transformer-based models demonstrate substantial ability to manage dialogue context and maintain coherent, context-aware interactions with users \citep{yue_survey_2025,antoniou_survey_2022}. Together, these advances have laid the foundation for generative AI systems and retrieval-augmented architectures that now dominate state-of-the-art QA research.

\section{Generative LLMs, RAG and Agents}

The latest advances in question answering are characterized by the emergence of generative LLMs and retrieval-augmented generation (RAG) architectures. In these systems, a retriever component dynamically searches external and continually updated knowledge bases, while a generator synthesizes fluent, grounded answers by conditioning on the retrieved information. This hybrid approach addresses many of the shortcomings of earlier QA models by significantly enhancing factual accuracy, contextual relevance, and system adaptability. Generative LLMs within the RAG framework are able to incorporate real-time knowledge, thereby reducing hallucinated content and providing up-to-date responses, even as external data sources evolve \citep{yue_survey_2025,lewis_retrieval-augmented_2020}. 

Furthermore, RAG-based QA systems offer practical advantages for scalability. Rather than requiring full model retraining to accommodate new information, they can simply update or expand the external document index or knowledge base. This design allows for the integration of vast and dynamic data resources, enabling high coverage across domains and rapid adaptation to new information needs. At the same time, these benefits come with trade-offs. RAG architectures require more complex infrastructure, including document indexing and retrieval pipelines, which increase computational overhead and system latency compared to traditional, static QA models. As a result, deploying and maintaining RAG-based systems can be more challenging, especially at scale.

Recent research also explores QA agents that operate as multi-stage pipelines and modular systems, capable of orchestrating question understanding, document retrieval, reasoning, and answer synthesis in a coordinated workflow. These agentic approaches often leverage LLMs for planning and reasoning, further expanding the capabilities and potential applications of QA technology.
\\


\autoref{tab:qa-comparison} summarizes the functional differences between traditional and RAG-based QA systems, highlighting the shift toward dynamic, retrieval-augmented, and generative approaches that characterize the current state of the discipline. The progression illustrates a profound transformation in how question answering is conceptualized, implemented, and applied across a wide range of domains.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X}
\toprule
\textbf{Feature} & \textbf{Traditional QAS} & \textbf{RAG QAS} \\
& \textit{(e.g., BERT, GPT-2/3)} & \textit{(Retriever + Generator)} \\
\midrule
Knowledge source & Fixed (training data) & Dynamic (external docs/databases) \\
Answer type & Extracted or generated & Retrieved + generated (synthesized) \\
Factual accuracy & Limited (can hallucinate or be outdated) & High (grounded in retrieved, up-to-date information) \\
Contextual depth & Limited & Comprehensive, nuanced \\
Scalability & Moderate & High (can update external data sources) \\
Computational cost & Lower & Higher (due to retrieval/generation) \\
Latency & Lower (faster for simple queries) & Higher (retrieval step adds time) \\
Complexity of setup & Simpler & More complex to maintain \\
Adaptability & Less adaptable to new domains & Highly adaptable via updated document index \\
\bottomrule
\end{tabularx}
\caption{Comparison of traditional vs. RAG question-answering systems.\\ \footnotesize{Adapted from \url{https://www.geeksforgeeks.org/nlp/rag-vs-traditional-qa/}.}}
\label{tab:qa-comparison}
\end{table}

\end{spacing}
