\chapter{The Evolution of Question-Answering Systems}
\label{chap:QAS}
\sloppy
\begin{spacing}{1.5}

This chapter introduces the foundations of question answering (QA) as both a computer science discipline and an applied task. Before the emergence of large language models (LLMs),\footnote{Large Language Models (LLMs) are advanced AI systems trained on massive text datasets to generate and understand human language. For an accessible overview, see \href{https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e}{\textit{A Very Gentle Introduction to Large Language Models without the Hype}} \citep{riedl_very_2023}.} Transformers,\footnote{The Transformer is a neural network architecture introduced in 2017 that efficiently models sequential data using a self-attention mechanism. The original paper, \textit{Attention Is All You Need} by Vaswani et al. (\citeyear{vaswani_attention_2017}), provides a foundational outline.} and modern generative AI,\footnote{Generative AI refers to systems capable of producing new content, such as text, images, or audio, based on learned patterns. For more, see the \textit{Stanford AI Index 2025 Report} \citep{maslej_artificial_2025}.} question-answering systems (QAS) progressed through distinct paradigms: from symbolic and rule-based architectures to classic information retrieval (IR) models and early neural networks approaches \citep{jurafsky_chapter_2024,antoniou_survey_2022}. Early systems depended on domain-specific adaptations, manually curated knowledge bases, keyword retrieval, and engineered features. In recent years, transformer-based language models such as BERT and GPT have significantly advanced the capabilities of QA systems by enabling both answer extraction and text generation. Unlike their predecessors, these models can generate or extract responses using deep contextual understanding derived from large-scale pretraining \citep{kaplan_scaling_2020}. However, they tend to exhibit factual inaccuracies, shallow contextual understanding in certain scenarios, and limited adaptability to new or evolving information. They also frequently hallucinate or generate outdated responses, constrained by their static training corpora \citep{harsh_comprehending_2024}.

\section{Pre-Transformer Era: Symbolic and Statistical Systems}
The development of QAS prior to the rise of Transformers was shaped by several key methodological shifts and technological milestones. These earliest efforts prioritised manually curated knowledge bases and rules-based systems for precise but limited question matching. As the scope of QA expanded, techniques evolved to incorporate large-scale information retrieval methods, statistical modeling, and increasingly complex approaches to feature engineering and answer extraction. This direction ultimately set the stage for early neural models that leveraged word embeddings and sequence modeling, gradually moving the discipline toward data-driven architectures and deeper semantic representation.

\subsection{Rule-Based Systems (1960s--1980s)}
Early QAS relied on highly constrained, domain-specific approaches built around manually constructed knowledge bases. These systems operated within carefully delineated boundaries, matching user questions to a limited set of predefined templates and answer patterns. While this design enabled highly precise responses in their target domains, it also rendered the systems brittle and inflexible -- minor variations in user queries or topics outside the encoded scope often resulted in failure to provide meaningful answers.

Expert systems from this era encoded explicit inference rules and logical representations of knowledge, enabling a form of automated reasoning that was fundamentally deterministic. However, these approaches struggled to address ambiguity or generalise beyond the hand-curated domain, and could not scale to larger, more dynamic information environments \citep{noauthor_question_2025, jurafsky_chapter_2024}.

Seminal examples of early domain-specific QA systems include:
\begin{itemize}
    \item \textbf{BASEBALL} (1960s): hand-coded rules and database logic for Major League Baseball\footnote{Major League Baseball (MLB) is the leading professional baseball league in North America. It is regarded as the world’s premier baseball competition.} questions \citep{green_baseball_1961};
    \item \textbf{SHRDLU}\footnote{SHRDLU was developed at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) between 1968--70. The software allowed users to interact conversationally with a program that could manipulate, describe, and answer questions about objects in a virtual \``blocks world\'', a simplified environment containing various movable blocks. Read more about SHRDLU program here: \url{https://hci.stanford.edu/winograd/shrdlu/}.} (late 1960s): symbolic reasoning for a blocks-world robot in a toy domain \textcolor{blue}{(Winograd, 1971)};
    \item \textbf{LUNAR} (1971): pattern matching and restricted knowledge base for geological questions about Moon rocks \citep{woods_lunar_1972};
    \item \textbf{Unix Consultant (UC)}\footnote{UC (QA) system, created at U.C. Berkeley (CA), answered queries about the Unix operating system using a hand-crafted knowledge base and could tailor responses to different user types \citep{robert_berkeley_1988}.}  and \textbf{LILOG}\footnote{LILOG project was as a text-understanding system designed for tourism information in a German city \citep{noauthor_question_2025}.} (1980s): domain-specific QA via linguistic rules and expert knowledge; though both projects remained at the demonstration stage, they contributed to advancing research in computational linguistics.
\end{itemize}

These early QA systems demonstrated the potential of automated question answering but highlighted the central challenge of balancing precision with generality and scalability. Their evolution would motivate the subsequent shift toward statistical and data-driven approaches \citep{jurafsky_chapter_2024, antoniou_survey_2022}.

\subsection{Classic Information Retrieval Strategies (1990s--mid-2010s)}
As the volume of unstructured web data grew, QA moved toward ranking text passages with IR techniques like TF-IDF\footnote{TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical method for ranking how important a word is to a document in a collection.} and BM25,\footnote{BM25 is a ranking function that improves information retrieval by considering term frequency, document length, and saturation effects.\\For more details on TF-IDF and BM25, read \textit{Introduction to Information Retrieval} \citep{manning_introduction_2008}.} to locate relevant content within large text collections. Open-domain QA systems -- such as those in TREC QA\footnote{TREC QA refers to the Question Answering track of the Text REtrieval Conference (TREC), a long-running evaluation series that has set benchmarks for open-domain QA research since 1999. See \url{https://trec.nist.gov/data/qa.html}\notecite{noauthor_text_nodate}} \citep{hirschman_natural_2001} -- shifted the focus from structured fact retrieval to returning ranked sentences or extracting answer spans from retrieved passages. These approaches made it possible to scale QA to a broad range of topics and data sources, yet they also introduced notable challenges. Lacking deep understanding of natural language, IR-based QA systems often failed to interpret nuances, synonyms, or complex phrasing, and frequently missed correct answers that did not explicitly match the user’s query terms \citep{antoniou_survey_2022, caballero_brief_2021}.

\subsection{Statistical Models and Feature Engineering (2000s--2018)}
During the 2000s and early 2010s, QA began to move beyond brittle rule-based systems. Instead of relying on hand-crafted heuristics alone, researchers increasingly turned to statistical methods capable of reasoning over large corpora. N-gram models and statistical IR techniques -- e.g., TF-IDF, BM25 and probabilistic models\footnote{Language Models for IR (LMIR) -- such as n-gram models -- estimate the probability of a query being generated by a document's language model. They capture local word dependencies and were widely used in early QA, speech recognition, and spelling correction \citep{ponte_language_1998}, but were later outperformed by models like RNNs, LSTMs, and Transformers due to their limited handling of long-range context} -- provided the first real capacity to navigate and rank massive text collections with some measure of relevance. By weighting terms according to their frequency and informativeness, these models made it possible to automatically surface candidate passages from unstructured data, a crucial step in scaling QA systems to the size of the web, large repositories and archives \citep{manning_introduction_2008}.

A major milestone of this period was IBM's \textit{Watson} system, which achieved notable success by winning the \textit{Jeopardy!} quiz competition in 2011.\footnote{The ``Jeopardy Challenge'' was a high-profile test where IBM \textit{Watson} competed on the American television quiz show \textit{Jeopardy!} against two of the show's greatest human champions. Watson’s victory demonstrated significant progress in machine comprehension and open-domain question answering (\href{https://en.wikipedia.org/w/index.php?title=IBM_Watson&oldid=1301611671}{Wikipedia IBM Watson}). In February 2013, IBM announced that \textit{Watson}'s first commercial deployment would assist with utilization management decisions for lung cancer treatment at Memorial Sloan Kettering Cancer Center in New York City, in partnership with WellPoint (now Elevance Health) \citep{upbin_ibms_2013}.} Watson’s \textit{DeepQA} architecture integrated hundreds of NLP, IR and ranking components, employing sophisticated pipelines to analyse and combine evidence from diverse sources \citep{ferrucci_building_2011}. However, despite its advanced design, \textit{Watson} relied on non-generative methods; it synthesised and ranked candidate answers but did not generate free-form responses from scratch.

Simultaneously, semantic QA systems also matured, mapping natural language (NL) questions to structured queries  -- e.g., using SPARQL -- executed over knowledge bases like Freebase and DBpedia. These systems required advanced components for entity recognition, relation extraction, and reasoning over symbolic representations. Typical architectures included steps like question analysis, sentence mapping, disambiguation, and query building, enabling automatic translation of NL into formal queries over RDF data sources. Thanks to the usage of ontology-mapping and linguistic resources -- e.g., WordNet \citep{miller_wordnet_1992} and BabelNet \citep{navigli_ten_2021} --, these approaches further bridged the gap between unstructured text and structured knowledge bases \citep{franco_ontology-based_2020}.

Throughout this period, feature engineering was the beating heart of QA. Techniques such as conditional random fields (CRFs) and support vector machines (SVMs) enabled models to exploit hand-crafted features -- including lexical overlap, question type, and answer patterns -- to enhance answer extraction from retrieved texts. Hybrid QA systems also appeared, combining keywords-based IR methods for unstructured sources with knowledge-base querying for fact-based answers, thereby improving both coverage and precision \citep{antoniou_survey_2022}.

This period, although still extractive and feature-dependent, set the stage for what followed. It demonstrated that scaling QA required both statistical reasoning over large corpora and semantic mapping into structured resources. All the while, it highlighted the bottlenecks of hand-engineered systems: they were labour-intensive to build, brittle across domains, and ultimately limited in their ability to capture deeper semantic relations. The gradual introduction of distributed word representations toward the end of this period hinted at a new trail, one that would come fully into focus with the neural architectures of the late 2010s.

\subsection{Early Neural and Generative Models (Late 2010s)}
The late 2010s marked a profound transition, as QA systems began to absorb the lessons of neural representation learning. The introduction of distributed word embeddings -- Word2Vec \citep{mikolov_efficient_2013}, GloVe \citep{pennington_glove_2014} and similar models -- shifted the paradigm from sparse statistical features to dense, continuous vector spaces. Instead of simply counting word overlaps, systems could now measure semantic proximity between terms, enabling them to recognise that, for example, ``excavation'' and ``dig'' refer to related concepts. This advance laid the groundwork for capturing meaning beyond surface forms, improving both retrieval and answer matching \citep{jurafsky_chapter_2024}.

Embedding representations enabled the rise of recurrent architectures, particularly recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRUs), which for the first time allowed systems to process language as sequences rather than bags of words. These models could in principle carry information across multiple tokens, making them attractive for reading comprehension tasks where the relation between question and passage unfolds over several sentences. Benchmarking datasets such as SQuAD \citep{rajpurkar_squad_2016} and Natural Questions \citep{kwiatkowski_natural_2019} became testing grounds for these methods, with LSTM-based encoders achieving state-of-the-art results by aligning question and context representations. Yet, the limitations quickly became evident: RNNs were notoriously poor at handling long-range dependencies, leading to failures when reasoning was required across multiple sentences, paragraphs, or documents \citep{jurafsky_chapter_2024}.

Around this time, researchers also began to experiment with generative models for QA, drawing inspiration from machine translation. Encoder-decoder architectures offered the tantalising possibility of producing answers as free-form text instead of merely extracting spans from source documents. These early generative QA systems demonstrated that models could synthesise responses in natural language, opening the door to more conversational applications. However, their outputs were often unreliable. Many simply rephrased the input passage, hallucinated details not grounded in evidence, or failed to maintain coherence when stitching together information from multiple contexts \citep{caballero_brief_2021}.

These developments set the stage for the subsequent breakthroughs brought about by attention mechanisms and transformer-based architectures, which dramatically improved the handling of context and factuality in generative QA.

\section{Blind Spots and Bottlenecks: The Shortcomings of Early Approaches}
Earlier approaches to question answering were hindered by several fundamental limitations. Most notably, symbolic and rule-based systems suffered from severe domain restrictions, as their performance relied on hand-crafted knowledge bases and rigid rules that did not generalise well to new or broader topics \citep{alqifari_question_2019}. The brittleness of these systems was further exposed by their heavy dependence on template matching, which frequently led to failures when users phrased questions in unanticipated ways or employed linguistic variations \citep{hirschman_natural_2001}. Statistical and IR models, while more scalable, continued to struggle with true semantic understanding and contextual reasoning, often retrieving only superficially relevant snippets in place of synthesising comprehensive or contextually rich answers \citep{alanazi_question_2021, diefenbach_core_2018}. The answers these systems produced were typically shallow, extracted verbatim from source texts rather than generated or adapted to the user’s specific information need \citep{hirschman_natural_2001,alqifari_question_2019}.

Substantial manual effort was required to design, maintain, and update rules, features, and parsers, creating significant bottlenecks and making adaptation to new domains costly and time-consuming \citep{alanazi_question_2021}. In addition, IR and knowledge base (KB) approaches frequently exhibited incomplete coverage, missing relevant answers due to differences in phrasing or limitations in their underlying datasets \citep{diefenbach_core_2018}. Early neural models, despite improvements, were generally confined to handling short text spans and struggled with complex or multi-sentence reasoning tasks. Finally, all these methods exhibited a strong dependence on the quantity and quality of available training data and engineered features, resulting in inconsistent performance across different domains and question types \citep{liu_challenges_2022,alanazi_question_2021,alqifari_question_2019,diefenbach_core_2018,hirschman_natural_2001}.

These cumulative factors left pre-generative systems largely inflexible and frail for QA purposes, with limited ability to provide context-aware, nuanced, or creative responses to user queries.

\section{Deep Learning Breakthroughs}
The advent of the Transformer architecture fundamentally reshaped the field of deep learning and revolutionised neural QA. Introduced by \citeauthor{vaswani_attention_2017} in 2017, Transformers replaced RNNs and LSTMs with a self-attention mechanism that could model relationships between words regardless of their distance in the input sequence. This innovation allowed for efficient parallelization during training and inference, greatly improving the scalability and performance of language models on a range of NLP tasks, including QA.

One of the earliest and most influential transformer-based models was BERT (Bidirectional Encoder Representations from Transformers) \citep{devlin_bert_2019}. BERT employs a bidirectional attention mechanism and is pretrained using a masked language modeling objective, allowing it to capture complex context from both directions in a sentence. When fine-tuned for QA benchmarks -- e.g., SQuAD --, BERT achieved unprecedented accuracy, reaching Exact Match and F1 scores above 85\% and 87\% respectively on the SQuAD 2.0 leaderboard, thus surpassing previous neural models and establishing a new standard for QA \citep{li_death_2024}.

Building on this foundation, subsequent models explored variations and enhancements of the Transformer paradigm. XLNet, for example, employed a permutation-based language modeling objective, enabling it to better capture bidirectional context and achieve state-of-the-art results on several QA benchmarks \citep{yang_xlnet_2020}. In specialised domains, models such as BioBERT extended the BERT architecture with additional pretraining on biomedical texts, achieving top performance on domain-specific challenges like the BioASQ competition \citep{yoon_pre-trained_2019}. Parallel research into model architectures also produced frameworks such as Dynamic Coattention Networks (DCN), which fused question and context representations through attention mechanisms and iterative decoding, further improving accuracy on reading comprehension tasks \citep{xiong_dynamic_2018}.

These breakthroughs ushered in a new research culture. QASs became systematically optimised at every stage, from tokenisation and embedding to retrieval and answer extraction \citep{farea_understanding_2025}. At the same time, the flexibility of Transformers encouraged exploration into conversational QA, multi-turn dialogue, and domain-specific fine-tuning \citep{yue_survey_2025}. Yet, for all their impact, transformer-based models still relied primarily on \textit{parametric memory}: their knowledge remained bounded by the data seen during pretraining. This limitation set the stage for a new class of approaches designed to bridge the gap between the static nature of models and the dynamic, real-world information needs.

\section{Large Language Models, Agents and Modular Pipelines}\label{sec:llm-agents}
Currently, a clear distinction emerges between ``traditional'' QA systems, primarily built upon general-purpose pretrained language models, and the new wave of modular approaches that dynamically retrieve external information sources. Traditional QA encompasses both extractive and generative paradigms, each defined by how they use the model’s internal knowledge. Extractive QA models are designed to identify and extract exact spans directly from a provided text or document, making them highly effective for fact-based questions and reading comprehension tasks. Generative QA models, in contrast, use natural language generation (NLG) to produce answers, typically synthesising or paraphrasing responses in ways that may not appear verbatim in the original text. However, despite their success, both of these paradigms are fundamentally limited by the static nature of their training data. They may struggle with rare, fast-changing, or domain-specific queries, and are prone to hallucinations\footnote{In the context of LLMs, hallucinations refer to outputs that are plausible-sounding but factually incorrect, fabricated, or unsupported by the underlying data or external sources \citep{harsh_comprehending_2024}.} and outdated information \citep{farea_understanding_2025}.

Recent advances in question answering are characterised by the emergence of retrieval-augmented generation (RAG). In these pipelines, a retriever component dynamically accesses external knowledge bases, while a generator conditions on the retrieved information to produce grounded answers. This approach addresses many of the shortcomings of earlier transformer-based models and significantly enhances factual accuracy, contextual relevance, and system adaptability. Generative LLMs within the RAG pipeline are able to incorporate real-time knowledge, thereby reducing hallucinated content and providing up-to-date responses, even as external data sources evolve \citep{yue_survey_2025,lewis_retrieval-augmented_2020}. Benchmarks show that RAG-enhanced models significantly outperform standard LLMs in factual QA, particularly in domains demanding precise recall or up-to-date knowledge. For instance, enterprise evaluations demonstrate up to 30-40\% improvement in incorporating domain-specific terminology compared to standalone models, while user trust increases substantially when source citations are included \citep{vaibhav_retrieval-augmented_2025}.

Furthermore, RAG-based QA systems offer practical advantages for scalability. Rather than requiring full model re-training to accommodate new information, they can simply update or expand the external KB. This design allows for the integration of vast and dynamic data resources, enabling high coverage across domains and rapid adaptation to new information needs. However, these benefits come with trade-offs. RAG architectures require more complex infrastructures, including document indexing and retrieval mechanisms, which increase operational overhead and latency compared to traditional, static QA systems. As a result, deploying and maintaining RAG-based systems can be more challenging, especially at scale.

Beyond RAG, a parallel evolution is visible in the emergence of LLM-based agents. Unlike monolithic models, these agents operate as orchestrators of multi-stage reasoning, combining planning, question understanding, retrieval, reasoning, and answer generation in an iterative loop. Architectures typically integrate memory to retain conversational context, planning modules to decide on next actions, and reasoning modules to balance internal ``thinking'' with external interactions, such as calling APIs, querying databases, or consulting humans \parencite{yue_survey_2025}. It overcomes the rigidity of earlier pipelines, which relied on static submodules trained in isolation, and the limitations of naive LLM QA, which lacks external grounding and dynamic adaptability. Agents thus introduce a form of controlled autonomy: they not only retrieve information but also decide \textit{when} and \textit{how} to engage tools, creating more flexible and resilient QA systems.

Current research highlights that these modular, agentic pipelines offer more than incremental improvements. They introduce transparency through source attribution, factual grounding, and explainability -- qualities increasingly demanded in high-stakes domains such as law, medicine, and cultural heritage. At the same time, promising directions include multimodal modes -- retrieving from text, images, or audio --, though cross-modal alignment remains an open challenge \citep{vaibhav_retrieval-augmented_2025}; hybrid retrieval that combines sparse lexical methods with dense search; and adaptive systems that dynamically tune retrieval and reasoning strategies based on query type and context (\cite{yue_survey_2025,vaibhav_retrieval-augmented_2025}). Taken together, these advances point toward a decisive shift: from static models locked within their parametric memory to dynamic, agentic systems capable of interacting with and reasoning over the evolving universe of human knowledge.

\autoref{tab:qa-comparison} summarises the functional differences between traditional and RAG-based QA systems, highlighting the shift toward dynamic, retrieval-augmented, and generative approaches that characterise the current state of the discipline. 

\addtocounter{table}{-1}
\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X}
\toprule
\textbf{Feature} & \textbf{Traditional QAS} & \textbf{RAG QAS} \\
& \textit{(e.g., BERT, GPT-2/3)} & \textit{(Retriever + Generator)} \\
\midrule
Knowledge source & Fixed (training data) & Dynamic (external docs/databases) \\
Answer type & Extracted or generated & Retrieved + generated (synthesised) \\
Factual accuracy & Limited (can hallucinate or be outdated) & High (grounded in retrieved, up-to-date information) \\
Contextual depth & Limited & Comprehensive, nuanced \\
Scalability & Moderate & High (can update external data sources) \\
Computational cost & Lower & Higher (due to retrieval/generation) \\
Latency & Lower (faster for simple queries) & Higher (retrieval step adds time) \\
Complexity of setup & Simpler & More complex to maintain \\
Adaptability & Less adaptable to new domains & Highly adaptable via updated document index \\
\bottomrule
\end{tabularx}
\vspace{0.5em}
\caption{Comparison of traditional vs. retrieval-augmented generation question-answering systems.\\ \footnotesize{Adapted from \url{https://www.geeksforgeeks.org/nlp/rag-vs-traditional-qa/}\nocite{noauthor_rag_2025}}}.
\label{tab:qa-comparison}
\end{table}


The main stages in the evolution of QA systems, along with representative approaches and landmark examples, are summarised in \autoref{tab:qa_evolution}.

\addtocounter{table}{-1}
\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash\bfseries}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
\toprule
\textbf{Models} & \textbf{QA Approach} & \textbf{Examples / Results}\\
\midrule
Symbolic / Rule-based (1960s–1980s) & Rule-based, domain-specific, handcrafted knowledge base & BASEBALL, LUNAR, SHRDLU \\
Early IR Approaches (1990s–mid-2010s) & Keyword retrieval, TF-IDF, BM25, open-domain ranking & TREC QA \\
Statistical / Seq2Seq (2000s–2018) & N-gram, embeddings, RNN/LSTM, statistical IR & Early neural QA, Reading comprehension in 2010s \\
Transformer-based & Pretraining, fine-tuning, self-attention & BERT (93\% F1 on SQuAD), XLNet \\
Generative LLMs and agents & Prompting, retrieval-augmented generation, agentic reasoning & GPT-3, RAG pipelines \\
\bottomrule
\end{tabularx}
\vspace{0.5em}
\caption{Evolution of question-answering systems}.
\label{tab:qa_evolution}
\end{table}

\end{spacing}
