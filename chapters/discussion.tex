\chapter{Discussion}
\label{chap:discussion}
\pdfcomment{WIP}
\begin{spacing}{1.5}

Explain what metrics values ~\% indicate for the GNA QA system.

Discuss trade-offs: precision vs. recall in heritage QA; speed vs. accuracy; LLM hallucination vs. retrieval grounding.

PROTOTYPE.

Overall, the prototype established:

the technical feasibility of combining dense retrieval with generative models for Italian-language queries;

the practical utility of combining human and LLM-based evaluation in the absence of gold standards;

the main bottlenecks, namely limited scalability, lack of chunk-level metadata control, and inefficiencies introduced by LangChainâ€™s orchestration.

LangChain was invaluable as scaffolding: it accelerated prototyping, unified stateful conversation with retrieval-augmented prompts, and smoothed integration with Mistral and Streamlit. Yet the same abstractions that sped up early development also hid critical details (error handling, call scheduling, metadata routing). Recognising this trade-off helped you carve out clearer module boundaries in the subsequent architecture and design an evaluation-first workflow that is easier to profile, ablate, and optimise.

\section{Strengths}
Scalable crawling and chunking pipeline.

Transparent retrieval with source citations.

Lightweight implementation (CPU-friendly, deployable on Streamlit).

\section{Weaknesses}

Limited adaptability to domain shifts, Maintenance and update strategy: the system is not designed to easily adapt to changes in the domain or knowledge base, which could affect its performance as the underlying information evolves.

EVALUATION PROTOCOL.
This methodology introduces its own limitations. Since questions were generated with the help of an LLM directly from document chunks, there is a risk of lexical leakage: wording and uncommon tokens in the question often mirror those of the gold chunk. This can unintentionally bias the retrieval stage, making the task easier for dense retrievers, which rely on semantic embeddings, while disadvantaging lexical retrievers, which benefit less from surface similarity. Moreover, the resulting queries may not faithfully reflect real user behaviour, which typically involves synonyms, abbreviations, typos, ellipses, or more implicit forms of reasoning. As such, while the synthetic datasets provided a controlled and reproducible environment for system evaluation, they should be interpreted as diagnostic tools rather than as complete proxies for deployment scenarios.

Security and privacy considerations: the prototype does not address potential security vulnerabilities or data privacy concerns associated with handling user queries and sensitive information.

\section{Relation to Previous Work}
Compare findings with studies in the bibliography; Show how this project aligns with or diverges from existing QA and RAG applications.

\section{Implications for Digital Humanities}
Value of RAG systems for cultural heritage platforms (accessibility, democratising archaeology knowledge).

Tension between automation and scholarly authority (machine-generated vs. curated responses).

Potential role as support tool for researchers, students, and the public.

\end{spacing}