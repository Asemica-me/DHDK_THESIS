\chapter{Discussion}
\label{chap:discussion}
\begin{spacing}{1.5}
The development and evaluation of the GNA AI assistant offer a valuable vantage point for reflecting on both the methodological intricacies of retrieval-augmented generation and its broader implications within cultural heritage contexts. What began as an attempt to address a specific practical challenge -- easing access to dense and highly specialised documentation -- gradually unfolded into a broader inquiry into how contemporary AI methods align, or fail to align, with the realities of scholarly and professional practice in archaeology and the digital humanities. The discussion that follows is not intended as a conclusive statement to frame the project as a success story or a cautionary failure; rather, it situates the results within their larger methodological, technical, and disciplinary context.

\section{Methodological Reflections}
The projectâ€™s two development phases, from prototype to full-scale implementation, trace an evolving methodological path. The prototype established the feasibility of pairing dense retrieval with generative AI to handle Italian-language queries, and demonstrated that, in the absence of authoritative gold standards, a custom evaluation strategy -- combining human annotation with LLM-based evaluation -- offered a workable, if imperfect, assessment strategy. At the same time, the prototype revealed critical bottlenecks. The dependence on LangChain undoubtedly accelerated prototyping stage, providing a convenient framework that unified stateful conversation, retrieval, and integration with Mistral and Streamlit. In parallel, its layers of abstraction came at a cost, introducing inefficiencies and concealing low-level processes such as API rate-limit handling and metadata routing. In this sense, the prototype fulfilled an essential role not simply by establishing technical feasibility, but also by acting as a diagnostic instrument, revealing the constraints of facilitated development frameworks and informing the methodological stance that guided the more mature implementation.

The re-engineered system refined these lessons into a more deliberate architectural framework. LangChain was replaced with custom modules and clearer component boundaries, together with an evaluation protocol that was more amenable to profile, ablate, and optimise. Here, the results foreground a series of trade-offs that are not merely technical but epistemic. Precision and recall in heritage QA translate into tensions between coverage and depth. Some users favour concise answers for quick consultation, while others expect fuller contextualisation; the system must balance these competing needs. In this context, results from ablation studies on retrieval reveal the uneven terrain of performance across different strategies. Hybrid approaches consistently proved the most reliable recipe, outperforming the baseline standalone dense retrieval. The score-blend variant, in particular, struck a productive balance between coverage and ranking quality, while retaining sub-second latency. These findings underline the pragmatic value of combining lexical and semantic signals in cultural heritage domains, where documents are rich in technical vocabulary but also require semantic generalisation. Likewise, they caution against uncritical reliance on techniques such as query rewriting and cross-encoder reranking, which in this setting introduced drawbacks and even depressed retrieval quality, partly due to the linguistic and domain mismatches with available cross-encoders.

User feedback painted a complementary perspective. A majority of responses were rated positively, with fluency and topical relevance consistently noted as strengths. These reactions suggest that, when sufficient contextual material was retrieved, the generative model was capable of producing responses that aligned well with user expectations in both style and substance. Still, a non-negligible portion of answers was judged incomplete or only partially satisfactory. This pattern points to retrieval obstructions -- rather than deficiencies in generative quality -- as the primary factor limiting user satisfaction. In other words, the weakest link in the pipeline is not the expressive power of the language model, which proved more than adequate in rendering information, but the adequacy of the retrieval step in capturing all the fragments required to compose a comprehensive and contextually grounded answer. The feedback thus reinforces a central lesson: in domain-specific QA, generation succeeds only insofar as retrieval delivers complete and precise evidence, with success depending less on linguistic fluency than on the adequacy of the retrieved material.

\section{Technical, Infrastructural and Ethical Considerations}
Deploying the system in a constrained hosting environment brought to light another layer of challenges that often remain implicit in AI research: infrastructure. Running a RAG-based QA system with multiple NLP resources on Streamlit Community Cloud, with its tight memory caps and CPU-only limitations, demanded aggressive optimisation. Lazy loading of embeddings, memory-mapped FAISS indices, batch processing, cache pruning, and garbage collection routines were not optional refinements but survival strategies. Even then, memory leaks emerged, requiring close inspection of leaking objects and eventually negotiations with the platform to secure extended resources. This experience brings to the surface a broader consideration, highlighting a critical but often neglected truth about the practical life of RAG systems, which depends as much on engineering choices and infrastructural adaptability as on algorithmic innovation.

The design of the GNA AI assistant was governed both by technical feasibility and by explicit commitments to transparency, licensing compliance, and auditability. Logs of evaluation runs, public release of code and configurations, and attention to the licensing status of all components collectively ensure that the system does not devolve into a ``black box'' artefact, opaque to its future users and maintainers. Privacy, too, is safeguarded by design. The system processes no personal data and relies exclusively on publicly accessible or openly licensed materials. These principles are not afterthoughts but foundational elements of trustworthiness, especially in the cultural heritage sector where provenance and accountability are intrinsic values.

Nevertheless, provenance remains an unfinished frontier. Although the system is designed to provide in-answer citations pointing to the retrieved chunks or source URLs, in practice this mechanism is fragile and at times inconsistent. This limitation marks an important avenue for future refinement, for without reliable citation grounding, the epistemic authority of the system remains limited. Indeed, a robust provenance layer is more than a cosmetic feature. In domains such as archaeology, where the authority of knowledge rests on traceability, it constitutes a meaningful requirement. Fluent responses alone do not suffice; in scholarly contexts, the provenance of those responses is often as important as the content itself. Strengthening this mechanism is therefore a central priority in the developmental roadmap.

\section{Evaluating Without a Net}
Since no authoritative gold standard exists for queries specific to the \textit{Geoportale Nazionale per l'Archeologia}, nor does a legacy system provide a baseline for comparison, evaluation relied on synthetic test sets and user ratings. Despite careful design, the evaluation protocol carries its own caveats. Generating evaluation queries with the help of an LLM introduced an inevitable degree of lexical leakage. This likely biased the task toward dense retrievers, which benefit from semantic similarity, while disadvantaging lexical strategies that rely less on token overlap. Moreover, real users rarely phrase queries in such clean, corpus-mirroring forms; they abbreviate, mistype, or ask obliquely. Thus, the synthetic datasets surely provided reproducibility and diagnostic clarity, but they cannot fully substitute for faithful simulations of practice and live deployment data.

The methods adopted highlighted general tendencies but fell short of capturing the nuanced requirements of archaeologists and heritage professionals in daily workflows. It is worth acknowledging the irony: in a research landscape overflowing with QA benchmarks -- ranging from early large-scale resources such as SQuAD, which defined the paradigm of span-based reading comprehension, to more compound datasets like HotpotQA, designed to test multi-hop reasoning across documents -- none align with the task at hand. These resources have unquestionably driven progress in NLU, establishing common baselines and enabling systematic comparison across models. Yet their scope remains fundamentally oriented toward general-purpose QA in English, often privileging cleanly segmented passages, unambiguous answers, and simplified contexts. In opposition, the richness and complexity of the GNA user manual -- with its blend of procedural guidelines, tables, and domain-specific jargon -- resist the assumptions embedded in these benchmarks. What emerges is a striking mismatch; while existing datasets are indispensable for model development, they fall short as instruments for evaluating systems designed to operate within the highly contextual workflows of archaeology and cultural heritage, which in this case are conducted in Italian and guided by practical application.


\section{Relation to Previous Work}
Positioning the GNA QA system within the broader research landscape of retrieval-augmented generation -- across question answering and adjacent domains -- brings into relief both continuities with existing approaches and points of departure. Recent literature documents a proliferation of RAG deployments across biomedical, legal, and cultural domains -- spanning enterprise settings and academic research alike -- each grappling with common challenges of retrieval quality, hallucination control, and evaluation \citep{agarwal_litllm_2025,bevara_prospects_2025,soman_observations_2024}. In this context, the GNA project aligns with a growing movement to adapt RAG pipelines to corpora rooted in a specialised domain, while also introducing constraints distinctive to cultural heritage infrastructures.

Methodologically, the system resonates with technical studies such as \citet{soman_observations_2024} work, which demonstrated the importance of chunking strategies and domain-specific terminology in retrieval accuracy. The RAG pipeline for the GNA similarly emphasised semantic preprocessing and careful segmentation of the operative manual, showing how small design choices directly affect system robustness. Like \citet{bevara_prospects_2025} in the library sciences, the GNA project foregrounded ethical and licensing considerations, where transparency and ethical accountability are understood as inseparable from technical performance.

Evaluation practices likewise reveal both alignment and shortcomings. Synthetic query generation, adopted here to compensate for the absence of gold standards, echoes \citet{bor-woei_generative_2024} argument for synthetic corpora in low resource domains. Yet it also introduced lexical leakage, potentially advantaging dense retrieval and diminishing the ecological validity of results -- an impediment less visible in biomedical and legal RAG applications, where annotated benchmarks (e.g., BioASQ) are available. The same limitations are noted in contemporary surveys \citep{yue_survey_2025}, reminding that evaluation methods must be interpreted with caution, and results contextualised against the artificiality of test conditions. By the way, the practical combination of automatic metrics, synthetic datasets, and user-centred ratings situates this project within a wider methodological turn toward hybrid evaluation protocols \citep{abeysinghe_challenges_2024,gupta_comprehensive_2024}.

Concurrently, the system exposes gaps and unresolved tensions that distinguish it from many existing RAG deployments. Whereas large-scale initiatives often aim for general-purpose scalability \citep{ramos-varela_context_2025,lala_paperqa_2023}, this project was deliberately scoped to a single, highly specialised corpus: the operative manual of \textit{Geoportale Nazionale per lâ€™Archeologia}. This narrow but dense domain called for balancing lexical and semantic retrieval methods, resulting in the demonstrated superiority of hybrid retrieval approaches. In this respect, the findings echo \citet{davis_unlocking_2025}, whose prototype for web archives similarly showed the advantages for retrieval precision of dense embeddings coupled with noise-reduction preprocessing, but they contrast with \citet{ramos-varela_context_2025}, who observed that large-context models could outperform RAG in querying multimodal museum collections. The lesson here is that performance is highly sensitive to corpus structure, query style, and domain vocabulary, framing RAG not as a one-size-fits-all solution, but as a family of adaptable techniques.

In the digital humanities specifically, the GNA AI assistant shares affinities with projects such as iREAL \citep{callaghan_prototyping_2025} and CAT-IA \citep{barbato_nasce_2025}, which likewise confront with the epistemic stakes of provenance and cultural sensitivity. Like these initiatives, it foregrounded the necessity of provenance-tracking and interpretability, though its own citation mechanism remains a provisional feature. This limitation underscores a broader challenge also identified in \citet{pollin_workshop_2024}, namely that without reliable grounding in source material, the danger for RAG is that of being perceived as obscure in its underlying processes, a risk particularly acute in heritage contexts where authority is inseparable from traceability.

Looked at in aggregate, these points of alignment and divergence position the GNA AI assistant as both participant in and contributor to ongoing debates. Its findings reinforce established lessons -- such as the value of hybrid retrieval and the indispensability of provenance -- while its domain, specific to the scope of technical documentation, highlights unresolved tensions around evaluation, scalability, and infrastructural constraints. Within the DH discourse, the system offers a modest but tangible case study, a reminder that applying RAG to cultural heritage entails more than merely porting established techniques into a new domain. It demands reworking them in light of epistemic values, infrastructural challenges, and humanistic priorities.

\section{Toward a Humanistic AI}
Beyond the technicalities, this project gestures toward wider implications for the digital humanities. The integration of AI systems into cultural heritage infrastructures is not merely an exercise in automation; it represents a deeper shift in how knowledge is organised, accessed, and mediated. In archaeology, where documentation is often layered, fragmented, and deeply contextual, the promise of retrieval-augmented generation lies in its capacity to interlace disparate traces of information into coherent forms of support, articulated through the lens of user queries. However, the risks of oversimplification and hallucination are ever present, demanding not blind adoption but careful stewardship.

This thesis does not claim that the GNA QA system possesses an understanding of archaeology, nor that it replaces the interpretive labour of scholars. Instead, it demonstrates how RAG pipelines can operate as mediating tools: structuring access, enhancing discoverability, and supporting human decision-making. The value of such systems is not measured by abstract notions of comprehension but by their utility in enabling more efficient, transparent, and contextually reliable engagement with cultural heritage data.

The trajectory traced here is not an end but an opening. The system as implemented constitutes a functioning end-to-end assistant, capable of live deployment in relation to the GNA ecosystem. Its contribution consists in demonstrating how methodological choices, infrastructural constraints, and ethical commitments intersect in the practical design of AI for the humanities. Beyond this, the project shows potential to restructure access to fragmented and technically dense knowledge, turning an intricate manual into answers that are more immediate and digestible. Its fragility, however, rests on careful parameter tuning, infrastructural support, and evaluation strategies sensitive to specific needs of the domain.

Future development may broaden the scope beyond the operative manual to archaeological datasets more generally, strengthen provenance mechanisms through reliable citation grounding, or explore multimodal integration by weaving text, images, and geospatial data into unified retrieval workflows.

Ultimately, the discussion circles back to the tension raised at the outset: the gap between technical ingenuity and the subtleties of human knowledge. Bridging this divide is less a matter of larger models or more data, than of cultivating a sustained dialogue between AI research and humanistic inquiry. The GNA AI assistant is not to be seen as a surrogate for scholarly interpretation, but as a mediator between users and the knowledge base, capable of efficiently surfacing relevant passages, shaping information flows, and enabling quicker orientation within complex documentation. In this sense, it offers a tangible contribution to the ongoing dialogue between AI and the humanities, pointing toward a future in which such systems act less as stand-ins for understanding and more as partners in the interpretation and tutelage of cultural heritage.

\end{spacing}